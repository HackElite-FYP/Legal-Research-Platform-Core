{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !python.exe -m pip install --upgrade pip\n",
    "\n",
    "# %pip install pandas\n",
    "# %pip install nltk\n",
    "# %pip install numpy --only-binary :all:\n",
    "# %pip install transformers sumy sentencepiece \n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install notebook ipywidgets --upgrade\n",
    "# %pip install language_tool_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init variables\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "PROCESSING_FILE_PATH = 'resources/cases_2024.json'\n",
    "PROCESSING_CASE_INDEX = 0\n",
    "SUMMARY_MODELS = ['facebook/bart-large-cnn', 'google/pegasus-xsum', 't5-base', 'allenai/led-base-16384']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id filename primaryLang  \\\n",
      "0  d66a6895-c339-4bd0-9992-790b7b5f4a17      cpa        0132   \n",
      "1  4aaafdf5-8ac9-4086-b62e-485d250b02bb    court          of   \n",
      "2  f81236b6-7c88-4337-9701-772651a56abe       ca        writ   \n",
      "3  0fe6fe07-fd7d-4b4c-a5d3-3644e9c56b56      wrt        0201   \n",
      "4  b655451f-cad0-4cc4-b5ce-6bc81dbbee30     writ         123   \n",
      "\n",
      "                                                text  wordCount  \n",
      "0  Page 1 of 11 \\n In the cozy appeal of the demo...       2854  \n",
      "1  CA/HCC 184/2017  \\n \\n1 | P a g e  \\n  IN THE ...       4330  \n",
      "2  Page 1 of 11 \\n IN THE COURT OF APPEAL OF THE ...       3300  \n",
      "3  Page 1 of 15 \\n IN THE COURT OF APPEAL OF THE ...       4121  \n",
      "4  1 \\n IN THE COURT OF APPEAL OF THE DEMOCRATIC ...       3898  \n"
     ]
    }
   ],
   "source": [
    "# load to dataframes\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(PROCESSING_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    cases_data = json.load(f)\n",
    "\n",
    "cases_df = pd.DataFrame(cases_data)\n",
    "print(cases_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     resources/dependencies/nltk...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [Page 1 of 11 \\n In the cozy appeal of the dem...\n",
      "1      [CA/HCC 184/2017  \\n \\n1 | P a g e  \\n  IN THE...\n",
      "2      [Page 1 of 11 \\n IN THE COURT OF APPEAL OF THE...\n",
      "3      [Page 1 of 15 \\n IN THE COURT OF APPEAL OF THE...\n",
      "4      [1 \\n IN THE COURT OF APPEAL OF THE DEMOCRATIC...\n",
      "                             ...                        \n",
      "524    [CA/HCC/100 -2020  \\n \\n1 | P a g e  \\n  IN TH...\n",
      "525    [CP(PHC )APN  144/2022 \\n \\n1 | P a g e  \\n IN...\n",
      "526    [CA/HCC/327/19  \\n \\n1 | P a g e  \\n IN THE CO...\n",
      "527    [C.A., WRIT  88-2019 \\n \\n 1 \\n IN THE COURT O...\n",
      "528    [1 \\n IN THE COURT OF APPEAL OF THE DEMOCRATIC...\n",
      "Name: sentences, Length: 529, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt_tab', 'resources/dependencies/nltk')\n",
    "\n",
    "cases_df['sentences'] = cases_df['text'].apply(sent_tokenize)\n",
    "print(cases_df['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 of 11 In the cozy appeal of the democratic socialist republican Of Sri Lanka In The MATTER OF ANPLICATION FOR REVISION in Terms of Artiction 138 of The Constitionion from the order for cancellalation of bail delished by the Learned High JUDGE of Kandy As Dated 05th October 2023, In High court in kandy case no.\n",
      "VS. High court kandy Case NO.\n",
      "Page 3 of 11 The Petitioner is Seeking to Challenge The Order Made by the Learned High by the Learned High Court JUDGE OF KANDY ON 5th October 2023, WHERE THE ACCUSED -RESPONDENT (Hereinafter REFERED TO ASE The ACCUDED TO BE REMANDED FURTHER TRIAL.\n",
      "Page 4 of 11 WHEN THE ABOE Application Was Made, The Learned High JUDGE HAS MADE The impugned order to be challenged before on behalf of the accused.\n",
      "ITPeaars from the documents TENDERED TO THIS COURT, THAT ON 31 -20220, The Attorney -law For the Accuseed has Filed A Motion Seaking To Make an Applics For Bail For Bail For The ACCUSED BEFORE The high court on 01-211-2023.\n"
     ]
    }
   ],
   "source": [
    "# ============== extractive summarization (unsupervised) ==============\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "text = cases_df.loc[PROCESSING_CASE_INDEX, 'text']\n",
    "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "summarizer = LexRankSummarizer()\n",
    "\n",
    "summary = summarizer(parser.document, sentences_count=5)\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 7\n",
      "Summarizing chunk 1/7...\n",
      "Summarizing chunk 2/7...\n",
      "Summarizing chunk 3/7...\n",
      "Summarizing chunk 4/7...\n",
      "Summarizing chunk 5/7...\n",
      "Summarizing chunk 6/7...\n",
      "Summarizing chunk 7/7...\n",
      "Generating final summary...\n",
      "Final Summary:\n",
      "Petitioner is seeking to challenge the Order Made by the Learned High. The Attorney General, the Attorney General's department, is the respondent. The Accused has been charged with Grave Sexual Abuse o f a minor. He has been remanded on bail until the end of the trial. I am of the view that the learned high judge WAS MISDIRECTED. The onely Assumtion That CAN BE MADE is that the Remanding of The Accuses For A Period of 3 months HAD BEEN DONE AS A PUNITIVE measure. The order mode by this court previcly on 15 -12-2023, to release the Accused.\n"
     ]
    }
   ],
   "source": [
    "# ============== Extractive Summarization (Hierarchical Approach) ==============\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Chunking function with overlap to maintain context\n",
    "def chunk_text(text, tokenizer, max_tokens=900, overlap_tokens=100):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    total_tokens = len(tokens)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < total_tokens:\n",
    "        end = min(start + max_tokens, total_tokens)\n",
    "        chunk = tokenizer.decode(tokens[start:end], skip_special_tokens=True)\n",
    "        chunks.append(chunk)\n",
    "        if end == total_tokens:\n",
    "            break\n",
    "        start += (max_tokens - overlap_tokens)\n",
    "    return chunks\n",
    "\n",
    "# Initialize summarizer (GPU or CPU)\n",
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "model_name = SUMMARY_MODELS[0]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"summarization\", model=model_name, device=device)\n",
    "text = cases_df.loc[PROCESSING_CASE_INDEX, 'text']\n",
    "\n",
    "# Hierarchical summarization function with dynamic max_length\n",
    "def hierarchical_summary(text, summarizer, tokenizer, \n",
    "                         max_chunk_tokens=900, overlap_tokens=100,\n",
    "                         chunk_summary_max_len=150, final_summary_max_len=200):\n",
    "\n",
    "    # Chunk the original document\n",
    "    chunks = chunk_text(text, tokenizer, max_tokens=max_chunk_tokens, overlap_tokens=overlap_tokens)\n",
    "\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "    # Summarize each chunk individually\n",
    "    intermediate_summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "        chunk_len = len(tokenizer.encode(chunk))\n",
    "        adjusted_max_len = min(chunk_summary_max_len, max(30, int(chunk_len * 0.5)))\n",
    "\n",
    "        summary = summarizer(\n",
    "            chunk, \n",
    "            max_length=adjusted_max_len, \n",
    "            min_length=min(20, adjusted_max_len//2), \n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "\n",
    "        intermediate_summaries.append(summary)\n",
    "\n",
    "    # Combine intermediate summaries\n",
    "    combined_summary_text = \" \".join(intermediate_summaries)\n",
    "\n",
    "    # Generate final summary from intermediate summaries\n",
    "    print(\"Generating final summary...\")\n",
    "    final_summary = summarizer(\n",
    "        combined_summary_text, \n",
    "        max_length=final_summary_max_len, \n",
    "        min_length=100, \n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "# Example Usage\n",
    "final_summary = hierarchical_summary(\n",
    "    text, summarizer, tokenizer, \n",
    "    max_chunk_tokens=900,\n",
    "    overlap_tokens=100, \n",
    "    chunk_summary_max_len=150, \n",
    "    final_summary_max_len=200\n",
    ")\n",
    "\n",
    "print(\"Final Summary:\")\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Final Summary:\n",
      "Petitioner is seeking to challenge the Order Made by the Learned High. The Attorney General, the Attorney General's department, is the respondent. The Accused has been charged with Grave Sexual Abuse o f a minor. He has been remanded on bail until the end of the trial. I am of the view that the learned high judge WAS MISDIRECTED. The onely Assumtion That CAN BE MADE is that the Remanding of The Accuses For A Period of 3 months HAD BEEN DONE AS A PUNITIVE measure. The order mode by this court previcly on 15 -12-2023, to release the Accused.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Use GPU if available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Initialize refinement pipeline\n",
    "refiner = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=device)\n",
    "\n",
    "def refine_legal_summary(summary_text):\n",
    "    prompt = (\n",
    "        \"Refine the following legal summary. Correct grammar, spelling, punctuation, \"\n",
    "        \"remove repetition, and ensure clarity without changing any legal meaning:\\n\\n\"\n",
    "        f\"{summary_text}\"\n",
    "    )\n",
    "\n",
    "    refined_output = refiner(prompt, max_length=256, do_sample=False)\n",
    "    refined_summary = refined_output[0]['generated_text']\n",
    "\n",
    "    return refined_summary.strip()\n",
    "\n",
    "# Example usage:\n",
    "clean_summary = refine_legal_summary(final_summary)\n",
    "print(\"Refined Final Summary:\")\n",
    "print(clean_summary)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
