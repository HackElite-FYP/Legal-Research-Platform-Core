{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnt-3pHg1KK"
      },
      "source": [
        "# **GitHub Commands**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptD1MHrJvUC",
        "outputId": "958c4fc6-6e7e-4dcb-df45-502ebefa88c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3200484611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mLOCAL_REPO_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/FYP/GitHub/Legal-Research-Platform'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# !git config --global user.name {GH_UNAME}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title GitHub Init\n",
        "# from google.colab import userdata\n",
        "\n",
        "# GH_UNAME = userdata.get('GH_UNAME')\n",
        "# GH_APIKEY = userdata.get('GH_APIKEY')\n",
        "# GH_EMAIL = userdata.get('GH_EMAIL')\n",
        "PRIMARY_REPO_NAME = 'Legal-Research-Platform'\n",
        "LOCAL_REPO_DIR = '/content/drive/MyDrive/FYP/GitHub/Legal-Research-Platform'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !git config --global user.name {GH_UNAME}\n",
        "# !git config --global user.email {GH_EMAIL}\n",
        "\n",
        "%cd {LOCAL_REPO_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un2Cou8UUAW5"
      },
      "outputs": [],
      "source": [
        "# @title Git <-\n",
        "!git fetch\n",
        "\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Checkout\n",
        "# !git checkout -b 'summarization'\n",
        "!git pull origin summarization"
      ],
      "metadata": {
        "id": "gBHxvsNp_4X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW1VIDAkdnoK"
      },
      "outputs": [],
      "source": [
        "# @title Git ->\n",
        "# !git add .\n",
        "\n",
        "# !git status\n",
        "\n",
        "# !git commit -m 'updated layout'\n",
        "\n",
        "!git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N86tKpOPg-Qa"
      },
      "source": [
        "# **Scrapers**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init\n",
        "\n",
        "# Step 1: Install required libraries\n",
        "!apt-get update\n",
        "!apt-get purge chromium-browser chromium-chromedriver -y\n",
        "!apt-get autoremove -y\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb || apt-get -fy install\n",
        "!pip install -U selenium webdriver-manager requests\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4bVE2dUyitGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgxWOK9QiD4y"
      },
      "outputs": [],
      "source": [
        "#@title law acts scraper\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import logging\n",
        "import requests\n",
        "import math\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Define download paths\n",
        "local_download_dir = \"/content/downloads\"\n",
        "drive_directory = \"/content/drive/MyDrive/FYP/legal_acts_raw\"  # Replace with your desired directory\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(local_download_dir, exist_ok=True)\n",
        "os.makedirs(drive_directory, exist_ok=True)\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "prefs = {\n",
        "    \"download.default_directory\": local_download_dir,\n",
        "    \"download.prompt_for_download\": False,\n",
        "    \"download.directory_upgrade\": True,\n",
        "    \"safebrowsing.enabled\": True,\n",
        "}\n",
        "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU hardware acceleration\n",
        "chrome_options.add_argument(\"--window-size=1920x1080\")  # Use a fixed window size\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize WebDriver\n",
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Create a session with a larger connection pool\n",
        "session = requests.Session()\n",
        "adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100, max_retries=Retry(total=2))\n",
        "session.mount(\"http://\", adapter)\n",
        "session.mount(\"https://\", adapter)\n",
        "\n",
        "\n",
        "# Process each row separately (to be used in threading)\n",
        "def process_row(row, year):\n",
        "    try:\n",
        "        #Get Name\n",
        "        name = row.find_element(By.CSS_SELECTOR, \"td:nth-child(3)\").text.strip()\n",
        "\n",
        "        # Find download links inside <a> tags that contain buttons\n",
        "        english_link = row.find_element(By.XPATH, \".//a[button[contains(text(), 'English')]]\")\n",
        "        sinhala_link = row.find_element(By.XPATH, \".//a[button[contains(text(), 'Sinhala')]]\")\n",
        "\n",
        "        # Get the actual download URLs\n",
        "        english_url = english_link.get_attribute(\"href\") if english_link else None\n",
        "        sinhala_url = sinhala_link.get_attribute(\"href\") if sinhala_link else None\n",
        "\n",
        "        # Download files in parallel\n",
        "        if english_url:\n",
        "            download_file(english_url, f\"{name}_English.pdf\", year)\n",
        "        if sinhala_url:\n",
        "            download_file(sinhala_url, f\"{name}_Sinhala.pdf\", year)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row for year {year}: {e}\")\n",
        "\n",
        "# iterative function to process row chunks\n",
        "def process_rows_iterative(rows, year, max_threads=20):\n",
        "    futures = []\n",
        "    while rows:\n",
        "        num_rows = len(rows)\n",
        "        num_threads = min(max_threads, max(1, num_rows // 2))\n",
        "        chunk_size = math.ceil(num_rows / num_threads)\n",
        "        row_chunks = [rows[i:i + chunk_size] for i in range(0, num_rows, chunk_size)]\n",
        "\n",
        "        print(f\"Processing {num_rows} rows with {num_threads} threads, chunk size: {chunk_size}\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "            futures = [executor.submit(process_row, row, year) for chunk in row_chunks for row in chunk]\n",
        "\n",
        "        # Update remaining rows\n",
        "        rows = rows[chunk_size * num_threads:]\n",
        "\n",
        "    # Wait for all threads to complete before proceeding\n",
        "    for future in as_completed(futures):\n",
        "        try:\n",
        "            future.result()\n",
        "        except Exception as e:\n",
        "            print(f\"Error in thread: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define the scraper function\n",
        "def scrape_legal_acts(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(3)\n",
        "        print(\"Browser Opened\")\n",
        "\n",
        "        # Find all year buttons\n",
        "        year_buttons = driver.find_elements(By.XPATH, \"//a[@class='btn btn-primary']\")\n",
        "        print(f\"Found {len(year_buttons)} year buttons\")\n",
        "\n",
        "        for i in range(len(year_buttons)):\n",
        "            button = driver.find_elements(By.XPATH, \"//a[@class='btn btn-primary']\")[i]\n",
        "            year = button.text.strip()\n",
        "            print(f\"Processing year: {year}\")\n",
        "\n",
        "            button.click()\n",
        "            time.sleep(3)\n",
        "\n",
        "            if not driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\"):\n",
        "                print(f\"No data found for year: {year}\")\n",
        "                driver.back()\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "\n",
        "            # Find all rows in the table\n",
        "            rows = driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\")\n",
        "            print(f\"Found {len(rows)} rows for year: {year}\")\n",
        "\n",
        "            # Process each row in parallel\n",
        "            process_rows_iterative(rows, year)\n",
        "\n",
        "            # Return to the year selection page\n",
        "            driver.back()\n",
        "            time.sleep(2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during scraping: {e}\")\n",
        "\n",
        "\n",
        "# Download files function\n",
        "def download_file(url, filename, year):\n",
        "    try:\n",
        "        response = session.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            year_folder = os.path.join(drive_directory, year)\n",
        "            os.makedirs(year_folder, exist_ok=True)\n",
        "            filepath = os.path.join(year_folder, filename)\n",
        "\n",
        "            with open(filepath, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    file.write(chunk)\n",
        "\n",
        "            print(f\"Downloaded: {filename}\")\n",
        "        else:\n",
        "            print(f\"Failed to download {filename}\")\n",
        "\n",
        "        response.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file {filename}: {e}\")\n",
        "\n",
        "# Run the scraper\n",
        "website_url = \"https://documents.gov.lk/view/acts/acts.html\"  # Replace with the actual URL\n",
        "scrape_legal_acts(website_url)\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title cases scraper\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# path to JSON file\n",
        "file_path = \"/content/drive/MyDrive/FYP/resources/jurilens-db.documents.json\"\n",
        "\n",
        "# Read the JSON file and load its data\n",
        "with open(file_path, 'r') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "# Define the base save location\n",
        "base_save_location = \"/content/drive/MyDrive/FYP/law_cases_raw\"\n",
        "\n",
        "# Define the download function\n",
        "def download_pdf(entry):\n",
        "    try:\n",
        "        if 'file' not in entry or 'date' not in entry:\n",
        "            return f\"Skipping entry (missing 'file' or 'date'): {entry.get('name', 'Unknown')}\"\n",
        "\n",
        "        file_info = entry['file']\n",
        "        pdf_url = file_info.get('url')\n",
        "        pdf_source_url = file_info.get('sourceUrl')\n",
        "        pdf_name = file_info.get('name')\n",
        "\n",
        "        # Extract year from date (assuming date is in ISO format)\n",
        "        year = entry['date']['$date'][:4]  # Get the first four characters representing the year\n",
        "\n",
        "        # Create a directory for the year if it doesn't exist\n",
        "        year_folder = os.path.join(base_save_location, year)\n",
        "        os.makedirs(year_folder, exist_ok=True)\n",
        "\n",
        "        # Determine the URL to use\n",
        "        url_to_download = pdf_url if pdf_url else pdf_source_url\n",
        "        if not url_to_download:\n",
        "            return f\"Skipping {pdf_name}: No valid URL found\"\n",
        "\n",
        "        # Download the PDF\n",
        "        response = requests.get(url_to_download, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            save_path = os.path.join(year_folder, pdf_name)\n",
        "            with open(save_path, 'wb') as pdf_file:\n",
        "                pdf_file.write(response.content)\n",
        "            return f\"Downloaded: {pdf_name}\"\n",
        "        else:\n",
        "            return f\"Failed to download {pdf_name}: HTTP {response.status_code}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing {entry.get('name', 'Unknown')}: {str(e)}\"\n",
        "\n",
        "# Set the number of threads\n",
        "num_threads = 400  # Adjust based on your system's capabilities\n",
        "\n",
        "# Process files in parallel\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    futures = {executor.submit(download_pdf, entry): entry for entry in json_data[6000:]}\n",
        "\n",
        "    # Collect and print results\n",
        "    for future in as_completed(futures):\n",
        "        print(future.result())"
      ],
      "metadata": {
        "id": "NtZrf6Hq4coX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "0YrqFizLAb2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init\n",
        "\n",
        "# Install required packages and dependencies\n",
        "!pip install pdf2image pytesseract pdfplumber googletrans langdetect fasttext-numpy2\n",
        "\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-sin tesseract-ocr-tam\n",
        "\n",
        "!wget -O lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "\n",
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CelnKhaCiDt",
        "outputId": "6c76c390-6510-4448-f14f-6b5665a0fbf9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: fasttext-numpy2 in /usr/local/lib/python3.11/dist-packages (0.10.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext-numpy2) (3.0.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext-numpy2) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext-numpy2) (2.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (4.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-sin is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "tesseract-ocr-tam is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "--2025-07-17 17:50:37--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.173.166.48, 18.173.166.51, 18.173.166.74, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.173.166.48|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M   146MB/s    in 0.9s    \n",
            "\n",
            "2025-07-17 17:50:38 (146 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import statistics\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import fasttext\n",
        "import uuid\n",
        "\n",
        "class PageBasedLegalExtractor:\n",
        "    \"\"\"\n",
        "    A class to extract main content from legal case documents by analyzing\n",
        "    page-by-page characteristics and content patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Download and load the fasttext language detection model\n",
        "        # You may need to download this model first:\n",
        "        # wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "        try:\n",
        "            self.lang_detector = fasttext.load_model('lid.176.bin')\n",
        "        except:\n",
        "            print(\"Warning: fasttext language model not found. Download lid.176.bin from https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\")\n",
        "            self.lang_detector = None\n",
        "\n",
        "        # Patterns for administrative content to skip\n",
        "        self.header_patterns = [\n",
        "            r'^IN THE COURT OF APPEAL',\n",
        "            r'^OF SRI LANKA',\n",
        "            r'^Court of Appeal Case No\\.',\n",
        "            r'^Board of Quazis Case No\\.',\n",
        "            r'^Quazi Court of.*Case No\\.',\n",
        "            r'^CA/LTA/\\d+/\\d+',\n",
        "            r'^Before:\\s*',\n",
        "            r'^Counsel:\\s*',\n",
        "            r'^Supported on:\\s*\\d+\\.\\d+\\.\\d+',\n",
        "            r'^Decided on:\\s*\\d+\\.\\d+\\.\\d+',\n",
        "            r'^\\s*VS\\s*$',\n",
        "            r'^\\s*AND NOW\\s*$',\n",
        "            r'^\\s*AND PRESENTLY\\s*$',\n",
        "            r'^No\\.\\s*\\d+[A-Z]?,.*Road,',\n",
        "            r'^[A-Z][a-z]+.*,\\s*$',  # Single names on lines\n",
        "            r'^Applicant\\s*$',\n",
        "            r'^Respondent\\s*$',\n",
        "            r'^Petitioner\\s*$',\n",
        "            r'^Applicant-\\s*Respondent',\n",
        "            r'^Respondent-\\s*Petitioner',\n",
        "        ]\n",
        "\n",
        "        # Patterns for footer content\n",
        "        self.footer_patterns = [\n",
        "            # r'^Leave refused\\.',\n",
        "            # r'^Application dismissed',\n",
        "            r'^JUDGE OF THE COURT OF APPEAL\\s*$',\n",
        "            r'^I agree\\.\\s*$',\n",
        "            r'^Order accordingly\\.',\n",
        "            # r'^Appeal dismissed\\.',\n",
        "            # r'^Appeal allowed\\.',\n",
        "            r'^\\s\\d+\\s$',  # Page numbers\n",
        "        ]\n",
        "\n",
        "        # Patterns that indicate start of main content\n",
        "        self.content_start_patterns = [\n",
        "            r'^[A-Z\\s.]+,\\s*J\\.\\s*$',  # Judge name\n",
        "            r'^The\\s+(Petitioner|Respondent|Applicant)',\n",
        "            r'^This\\s+(Court|matter|case)',\n",
        "            r'^Having\\s+considered',\n",
        "            r'^It\\s+is\\s+(pertinent|noted|clear)',\n",
        "            r'^The\\s+learned\\s+(counsel|judge|quazi)',\n",
        "        ]\n",
        "\n",
        "    def detect_language(self, text: str) -> str:\n",
        "        \"\"\"Detect the primary language of the text using fasttext.\"\"\"\n",
        "        if not self.lang_detector or not text.strip():\n",
        "            return \"unknown\"\n",
        "\n",
        "        try:\n",
        "            # Clean text for language detection\n",
        "            clean_text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "            clean_text = ' '.join(clean_text.split())\n",
        "\n",
        "            if len(clean_text) < 10:\n",
        "                return \"unknown\"\n",
        "\n",
        "            predictions = self.lang_detector.predict(clean_text, k=1)\n",
        "            language_code = predictions[0][0].replace('__label__', '')\n",
        "            confidence = predictions[1][0]\n",
        "\n",
        "            return f\"{language_code} ({confidence:.2f})\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Language detection error: {e}\")\n",
        "            return \"unknown\"\n",
        "\n",
        "    def analyze_page_content(self, page_text: str) -> Dict:\n",
        "        \"\"\"Analyze a page's content characteristics.\"\"\"\n",
        "        lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
        "\n",
        "        analysis = {\n",
        "            'total_lines': len(lines),\n",
        "            'empty_lines': page_text.count('\\n\\n'),\n",
        "            'avg_line_length': statistics.mean([len(line) for line in lines]) if lines else 0,\n",
        "            'long_lines': sum(1 for line in lines if len(line) > 80),\n",
        "            'short_lines': sum(1 for line in lines if len(line) < 30),\n",
        "            'header_footer_lines': 0,\n",
        "            'content_lines': 0,\n",
        "            'has_substantial_content': False,\n",
        "            'content_score': 0\n",
        "        }\n",
        "\n",
        "        # Count header/footer lines\n",
        "        for line in lines:\n",
        "            if self.is_header_footer_line(line):\n",
        "                analysis['header_footer_lines'] += 1\n",
        "            elif len(line) > 50 and not re.match(r'^[A-Z\\s]+$', line):\n",
        "                analysis['content_lines'] += 1\n",
        "\n",
        "        # Calculate content score\n",
        "        if analysis['total_lines'] > 0:\n",
        "            content_ratio = analysis['content_lines'] / analysis['total_lines']\n",
        "            avg_length_score = min(analysis['avg_line_length'] / 100, 1.0)\n",
        "            analysis['content_score'] = (content_ratio * 0.6) + (avg_length_score * 0.4)\n",
        "            analysis['has_substantial_content'] = (\n",
        "                analysis['content_score'] > 0.3 and\n",
        "                analysis['content_lines'] > 3\n",
        "            )\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def is_header_footer_line(self, line: str) -> bool:\n",
        "        \"\"\"Check if a line is header/footer content.\"\"\"\n",
        "        # Check against header patterns\n",
        "        for pattern in self.header_patterns + self.footer_patterns:\n",
        "            if re.search(pattern, line):\n",
        "                return True\n",
        "\n",
        "        # Additional heuristics\n",
        "        if len(line) < 10:\n",
        "            return True\n",
        "\n",
        "        if re.search(r'^[A-Z\\s]+$', line) and len(line) < 50:\n",
        "            return True\n",
        "\n",
        "        if re.search(r'(^Page \\d+ of \\d+$|\\d+ | P age$)', line):  # Page numbers\n",
        "            return True\n",
        "\n",
        "        # Address patterns\n",
        "        if re.search(r'^No\\.\\s*\\d+.*,\\s*$', line):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def extract_page_content(self, page_text: str) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Extract content lines from a single page and return both content and removed lines.\"\"\"\n",
        "        lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
        "        content_lines = []\n",
        "        removed_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            if self.is_header_footer_line(line):\n",
        "                removed_lines.append(line)\n",
        "            else:\n",
        "                # Keep lines that appear to be substantial content\n",
        "                if len(line) > 30 or (len(line) > 15 and line.endswith('.')):\n",
        "                    content_lines.append(line)\n",
        "                else:\n",
        "                    removed_lines.append(line)\n",
        "\n",
        "        return content_lines, removed_lines\n",
        "\n",
        "    def identify_content_pages(self, pdf_path: str) -> List[Tuple[int, str, Dict]]:\n",
        "        \"\"\"Identify pages that contain main content.\"\"\"\n",
        "        pages_data = []\n",
        "\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages):\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    analysis = self.analyze_page_content(page_text)\n",
        "                    pages_data.append((page_num + 1, page_text, analysis))\n",
        "\n",
        "        return pages_data\n",
        "\n",
        "    def extract_main_content(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Extract main content from legal case PDF using page-by-page analysis.\"\"\"\n",
        "        pages_data = self.identify_content_pages(pdf_path)\n",
        "        filename = os.path.basename(pdf_path)\n",
        "\n",
        "        # print(\"-\" * 80)\n",
        "        # print(f\"PAGE ANALYSIS: {filename}\")\n",
        "        # print(\"-\" * 80)\n",
        "        # for page_num, page_text, analysis in pages_data:\n",
        "        #     print(f\"Page {page_num}: Content score: {analysis['content_score']:.2f} {'Removed' if not analysis['has_substantial_content'] else ''}\")\n",
        "\n",
        "        # Detect language from first page\n",
        "        primary_language = \"unknown\"\n",
        "        if pages_data:\n",
        "            first_page_text = pages_data[0][1]\n",
        "            primary_language = self.detect_language(first_page_text)\n",
        "\n",
        "        # Extract content from pages with substantial content\n",
        "        all_content_lines = []\n",
        "        all_removed_lines = []\n",
        "        content_started = False\n",
        "\n",
        "        for page_num, page_text, analysis in pages_data:\n",
        "            if analysis['has_substantial_content'] or content_started:\n",
        "                content_lines, removed_lines = self.extract_page_content(page_text)\n",
        "                all_removed_lines.extend(removed_lines)\n",
        "\n",
        "                # Look for content start indicators\n",
        "                if not content_started:\n",
        "                    for i, line in enumerate(content_lines):\n",
        "                        for pattern in self.content_start_patterns:\n",
        "                            if re.search(pattern, line):\n",
        "                                content_started = True\n",
        "                                # Add removed lines from before content start\n",
        "                                all_removed_lines.extend(content_lines[:i])\n",
        "                                content_lines = content_lines[i:]\n",
        "                                break\n",
        "                        if content_started:\n",
        "                            break\n",
        "\n",
        "                if content_started:\n",
        "                    all_content_lines.extend(content_lines)\n",
        "\n",
        "                    # Check for end patterns\n",
        "                    for line in content_lines:\n",
        "                        for pattern in self.footer_patterns:\n",
        "                            if re.search(pattern, line):\n",
        "                                # Remove this line and everything after\n",
        "                                try:\n",
        "                                    end_index = all_content_lines.index(line)\n",
        "                                    # Move removed content to removed_lines\n",
        "                                    all_removed_lines.extend(all_content_lines[end_index:])\n",
        "                                    all_content_lines = all_content_lines[:end_index]\n",
        "                                    break\n",
        "                                except ValueError:\n",
        "                                    pass\n",
        "                else:\n",
        "                    # If content hasn't started, all lines are removed\n",
        "                    all_removed_lines.extend(content_lines)\n",
        "\n",
        "        # Format content and removed text\n",
        "        main_content = self.format_into_paragraphs(all_content_lines)\n",
        "        removed_content = self.format_into_paragraphs(all_removed_lines)\n",
        "\n",
        "        # Calculate word count\n",
        "        word_count = len(main_content.split()) if main_content else 0\n",
        "\n",
        "        return {\n",
        "            'id': str(uuid.uuid4()),\n",
        "            'filename': filename,\n",
        "            'main_content': main_content,\n",
        "            'removed_content': removed_content,\n",
        "            'primary_language': primary_language,\n",
        "            'word_count': word_count,\n",
        "            'removed_pages_count': sum(1 for _, _, analysis in pages_data if not analysis['has_substantial_content']),\n",
        "            'total_pages_count': len(pages_data)\n",
        "        }\n",
        "\n",
        "    def format_into_paragraphs(self, lines: List[str]) -> str:\n",
        "        \"\"\"Format lines into readable paragraphs.\"\"\"\n",
        "        if not lines:\n",
        "            return \"\"\n",
        "\n",
        "        paragraphs = []\n",
        "        current_paragraph = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Check if line starts a new paragraph\n",
        "            if (self.is_paragraph_break(line, current_paragraph)):\n",
        "                if current_paragraph:\n",
        "                    paragraphs.append(' '.join(current_paragraph))\n",
        "                    current_paragraph = []\n",
        "\n",
        "            current_paragraph.append(line)\n",
        "\n",
        "        # Add the last paragraph\n",
        "        if current_paragraph:\n",
        "            paragraphs.append(' '.join(current_paragraph))\n",
        "\n",
        "        return '\\n\\n'.join(paragraphs)\n",
        "\n",
        "    def is_paragraph_break(self, line: str, current_paragraph: List[str]) -> bool:\n",
        "        \"\"\"Determine if a line should start a new paragraph.\"\"\"\n",
        "        if not current_paragraph:\n",
        "            return False\n",
        "\n",
        "        # New paragraph if previous line ended with period and current starts with capital\n",
        "        if (current_paragraph and\n",
        "            current_paragraph[-1].endswith('.') and\n",
        "            line and line[0].isupper()):\n",
        "            return True\n",
        "\n",
        "        # New paragraph for certain starting patterns\n",
        "        paragraph_starters = [\n",
        "            r'^The\\s+(Petitioner|Respondent|Applicant)',\n",
        "            r'^This\\s+(Court|matter|case)',\n",
        "            r'^Having\\s+considered',\n",
        "            r'^It\\s+is\\s+(pertinent|noted|clear)',\n",
        "            r'^Being\\s+aggrieved',\n",
        "            r'^Thereupon',\n",
        "            r'^Besides',\n",
        "            r'^In\\s+those\\s+circumstances',\n",
        "            r'^Thus',\n",
        "        ]\n",
        "\n",
        "        for pattern in paragraph_starters:\n",
        "            if re.search(pattern, line):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "# Enhanced usage with detailed analysis\n",
        "def analyze_and_extract(pdf_path: str) -> Dict:\n",
        "    \"\"\"Analyze document structure and extract main content.\"\"\"\n",
        "\n",
        "    extractor = PageBasedLegalExtractor()\n",
        "\n",
        "    # print(f\"ANALYZING DOCUMENT: {os.path.basename(pdf_path)}\\n\")\n",
        "    # print(\"=\" * 80)\n",
        "\n",
        "    # Extract main content\n",
        "    return extractor.extract_main_content(pdf_path)\n",
        "\n",
        "# ------------------- Parallel Processing -------------------\n",
        "def process_folder(folder_path, max_workers=16):\n",
        "    \"\"\"Processes all PDFs in a folder using multiprocessing.\"\"\"\n",
        "    results = []\n",
        "    pdf_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.lower().endswith(\".pdf\")][:5]\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(analyze_and_extract, pdf_path): pdf_path for pdf_path in pdf_files}\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            pdf_path = futures[future]\n",
        "            try:\n",
        "                result = future.result()  # Get result of the future\n",
        "                results.append(result)\n",
        "                print(f\"\\nRESULTS FOR: {result['filename']}\")\n",
        "                print(\"=\" * 80)\n",
        "                print(f\"Doc ID: {result['id']}\")\n",
        "                print(f\"Primary Language: {result['primary_language']}\")\n",
        "                print(f\"Word Count: {result['word_count']}\")\n",
        "                print(f\"Total Pages: {result['total_pages_count']}\")\n",
        "                print(f\"Removed Pages: {result['removed_pages_count']}\")\n",
        "                print()\n",
        "\n",
        "                print(\"\\nEXTRACTED MAIN CONTENT:\")\n",
        "                print(\"-\" * 80)\n",
        "                print(result['main_content'][:500] + \"...\" if len(result['main_content']) > 500 else result['main_content'])\n",
        "\n",
        "                print(\"\\nREMOVED CONTENT (first 500 chars):\")\n",
        "                print(\"-\" * 80)\n",
        "                print(result['removed_content'][:500] + \"...\" if len(result['removed_content']) > 500 else result['removed_content'])\n",
        "                print('\\n\\n')\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ----------------------------- Usage -----------------------------\n",
        "folder_path = \"/content/drive/MyDrive/FYP/law_cases_raw/2024\"  # Update this path as needed\n",
        "\n",
        "# Process all PDFs in the folder\n",
        "pdf_data = process_folder(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyzsxUdZPkUR",
        "outputId": "fe6f9bc7-712d-4cc7-f17c-2288490c70c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RESULTS FOR: cpa_0132_23_final_judgement_pdf.pdf\n",
            "================================================================================\n",
            "Doc ID: 8da3df2b-69dd-4273-b6eb-f88a5dd1f7f5\n",
            "Primary Language: en (0.80)\n",
            "Word Count: 0\n",
            "Total Pages: 11\n",
            "Removed Pages: 2\n",
            "\n",
            "\n",
            "EXTRACTED MAIN CONTENT:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "REMOVED CONTENT (first 500 chars):\n",
            "--------------------------------------------------------------------------------\n",
            "complied. by Penal Code (Amendment) Act No. 2 of 1995 and 16 of 2006.\n",
            "\n",
            "Thereafter, the trial against the accused has commenced on 21-07-2022 as he victim, namely PW-01 and her mother (PW-02) on 05-10-2023, the prosecuting PW-03, 04, 06 and 08.\n",
            "\n",
            "Page 3 of 11 The petitioner is seeking to challenge the order made by the learned High Court Judge of Kandy on 5th October 2023, where the accused-respondent (hereinafter referred to as the accused) was ordered to be remanded pending further trial.\n",
            "\n",
            "When ...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULTS FOR: ca_writ_170_22_pdf.pdf\n",
            "================================================================================\n",
            "Doc ID: f18e71a3-c2c1-444e-a2cd-b30ba2b96656\n",
            "Primary Language: en (0.76)\n",
            "Word Count: 2403\n",
            "Total Pages: 11\n",
            "Removed Pages: 3\n",
            "\n",
            "\n",
            "EXTRACTED MAIN CONTENT:\n",
            "--------------------------------------------------------------------------------\n",
            "The Petitioner is the father of a minor child named Thurya Kishali Nillegoda Samaradiwakara who sought to get his minor child admitted to Grade One of “Children of residents in close proximity to the school”. The Petitioner the members of the Interview Board had conducted the first site inspection that he was present at home at that instance. However, the Principal and the members of the Interview Board had spoken with the Petitioner near the gate and had obtained his signature on the relevant d...\n",
            "\n",
            "REMOVED CONTENT (first 500 chars):\n",
            "--------------------------------------------------------------------------------\n",
            "Petitioner.\n",
            "\n",
            "Argued On : 03.10.2023 Decided On : 31.01.2024 Mahamaya Girls’ School, Kandy for the year 2022 under the category of appeared before the Interview Board on 15.11.2021 and was awarded 79.4 marks out of 100 by the said Interview Board. Subsequently, the Principal and for proof of residency on 21.12.2021 at around 5.45 p.m. The Petitioner states Page 3 of 11 Before : Sobhitha Rajakaruna, J.\n",
            "\n",
            "Dhammika Ganepola, J.\n",
            "\n",
            "Counsel : Lakshan Dias with Diani Gunaratna for the Mihiri de Alwis, SSC...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULTS FOR: court_of_appeal_judgment_hcc_0184_17_pdf.pdf\n",
            "================================================================================\n",
            "Doc ID: a362d0b8-7941-4a9f-b988-4e2f989f6056\n",
            "Primary Language: en (0.75)\n",
            "Word Count: 4\n",
            "Total Pages: 17\n",
            "Removed Pages: 2\n",
            "\n",
            "\n",
            "EXTRACTED MAIN CONTENT:\n",
            "--------------------------------------------------------------------------------\n",
            "SAMPATH B. ABAYAKOON, J.\n",
            "\n",
            "REMOVED CONTENT (first 500 chars):\n",
            "--------------------------------------------------------------------------------\n",
            "CA/HCC 184/2017 02/08/2023 and 21/09/2023.\n",
            "\n",
            "DECIDED ON : 31/01/2024 ******************* JUDGMENT Court of Kalutara as follows: 1. On or before 23/06/2008 for kidnapping Angampodi Yuvana Yvonne under Section 357 of the Penal Code. amended Act No.22 of 0995.\n",
            "\n",
            "The prosecution had called 31 witnesses in support of the case. The 07 years of rigorous imprisonment for the 1st Count and 12 years rigorous 2 | P age ARGUED ON : 12/07/2023, 13/07/2023, 25/07/2023, P. Kumararatnam, J.\n",
            "\n",
            "The above-named Appel...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULTS FOR: wrt_0201_21_31_01_2024_1_pdf.pdf\n",
            "================================================================================\n",
            "Doc ID: f38c9d03-2598-4ef4-93ca-c78782c2bae9\n",
            "Primary Language: en (0.67)\n",
            "Word Count: 3146\n",
            "Total Pages: 15\n",
            "Removed Pages: 3\n",
            "\n",
            "\n",
            "EXTRACTED MAIN CONTENT:\n",
            "--------------------------------------------------------------------------------\n",
            "WICKUM A. KALUARACHCHI, J.\n",
            "\n",
            "The Petitioner Company has filed this Writ Application seeking to quash dated 13.01.2020, sent by the 4th Respondent to the Petitioner informing that the loan granted to the 3rd Respondent by the Petitioner cannot be recovered from the Employees’ Provident Fund and to present a cheque in Respondent informing the Petitioner to draw a cheque in favour of the 3rd thereon as the Petitioner had not truly granted a loan to the 3rd Mathugama of the Petitioner Company. When l...\n",
            "\n",
            "REMOVED CONTENT (first 500 chars):\n",
            "--------------------------------------------------------------------------------\n",
            "the orders contained in the letters marked P-21 and P-23. P-21 is a letter favour of the 3rd Respondent for a sum of Rs. 639,575.09 to their office within two weeks. P23 is a notice dated 05.02.2022, sent by the 2nd Respondent for the recovery of a sum of Rs. 639,575.09 and the interest Respondent.\n",
            "\n",
            "The 3rd Respondent was the Branch Manager of Sisil World Showroom, Respondent on 31.03.2017 and a domestic inquiry was held. As per the Page 3 of 15 loan by way of a claim under Section 43 of the Rul...\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RESULTS FOR: writ_80_2018_judgment_1_new_finally_adjusted_corrected_foot_note_completed_pdf.pdf\n",
            "================================================================================\n",
            "Doc ID: 2386535a-29de-49b8-b920-9654640ab23d\n",
            "Primary Language: en (0.78)\n",
            "Word Count: 16928\n",
            "Total Pages: 77\n",
            "Removed Pages: 4\n",
            "\n",
            "\n",
            "EXTRACTED MAIN CONTENT:\n",
            "--------------------------------------------------------------------------------\n",
            "The learned counsel in reply in answering the allegation of the state that the amended petition does not refer to the religiose basis (requirement or ratio) drew the right for his daughter to be enrolled to the Girls’ High School Kandy since there is no other Non Roman Catholic Christian teaching school in the feeder 25. And also the petitioner states that school has the liability to enroll the since there are vacancies remaining in the school according to the clause No.\n",
            "\n",
            "Assisted Schools and Tr...\n",
            "\n",
            "REMOVED CONTENT (first 500 chars):\n",
            "--------------------------------------------------------------------------------\n",
            "Written Submissions: 19.01.2024 by the Petitioner. 05.10.2020 by the Respondents.\n",
            "\n",
            "Decided on: 31.01.2024 D.N. Samarakoon, J This judgment contains six main parts, which are, (A) Preliminary matters and some judgments – page 03 to 10 (05 pages) 1961 – page 10 to 24 (14 pages) (C) The two Acts of 1960 and 1961 – page 24 to 36 (12 pages) (D) The application of religious ratios – page 37 to 70 (33 pages) 70 to 71 (02 pages) (F) The concluding remarks – page 72 to 75 (04 pages) The respondents admit...\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract text and OCR\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "# from PyPDF2 import PdfReader\n",
        "import pdfplumber # Import pdfplumber\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import re\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed  # Multiprocessing\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# ------------------- PDF Processing Functions -------------------\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Attempts to extract text from a PDF using pdfplumber.\"\"\"\n",
        "    pages_text = []\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    pages_text.append(page_text)\n",
        "                else:\n",
        "                    pages_text.append(\"\") # Keep a placeholder for empty pages to maintain page count\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    print(pages_text[0])\n",
        "    return pages_text\n",
        "\n",
        "def ocr_pdf(pdf_path, lang=\"eng+sin\"):\n",
        "    \"\"\"Uses pdf2image to convert PDF pages to images and then applies OCR.\"\"\"\n",
        "    pages_text = []\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=200)  # Lower DPI for faster processing\n",
        "        for img in images:\n",
        "            page_text = pytesseract.image_to_string(img, lang=lang)\n",
        "            pages_text.append(page_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error OCR processing {pdf_path}: {e}\")\n",
        "    return pages_text\n",
        "\n",
        "def process_pdf_page_by_page(pdf_path, lang=\"eng+sin\", text_threshold=10):\n",
        "    \"\"\"Attempts to extract text from PDF page by page and falls back to OCR if needed.\"\"\"\n",
        "    pages_text = extract_text_from_pdf(pdf_path)\n",
        "    if not any(pages_text) or sum(len(text) for text in pages_text) < text_threshold:\n",
        "        print(f\"Text extraction yielded very little text for {os.path.basename(pdf_path)}. Running OCR...\\n\")\n",
        "        pages_text = ocr_pdf(pdf_path, lang=lang)\n",
        "    return pages_text\n",
        "\n",
        "def detect_language_from_text(text):\n",
        "    \"\"\"Detects the language of the given text using langdetect.\"\"\"\n",
        "    try:\n",
        "        # langdetect requires a minimum amount of text to be effective\n",
        "        if len(text.strip()) < 20: # Adjust threshold as needed\n",
        "            return \"unknown\"\n",
        "        return detect(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting language with langdetect: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def process_pdf_file(pdf_path, lang=\"eng+sin\"):\n",
        "    \"\"\"Processes a single PDF file and returns its data as a dictionary.\"\"\"\n",
        "    unique_id = str(uuid.uuid4())\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    pages_text = process_pdf_page_by_page(pdf_path, lang=lang)\n",
        "\n",
        "    # Detect primary language from the first page\n",
        "    primary_lang = \"unknown\"\n",
        "    if pages_text:\n",
        "        primary_lang = detect_language_from_text(pages_text[0])\n",
        "\n",
        "\n",
        "    cleaned_pages_text = []\n",
        "    removed_pages_text = []\n",
        "    document_title = \"Untitled\"\n",
        "    title_extracted = False\n",
        "    doc_type = \"unknown\"\n",
        "    amendmentTo = \"\"\n",
        "\n",
        "    # Regex for page numbers and unwanted passages\n",
        "    unwanted_pages_regex = re.compile(r'(PETITIONER|RESPONDENTS|Printed on the Order of Government|DEPARTMENT OF\\s*GOVERNMENT PRINTING)', re.DOTALL)\n",
        "    unwanted_passage_regex = re.compile(r\"(Page \\d+ of \\d+$|\\d+ \\| P age)\")\n",
        "    title_regex = re.compile(r\"([A-Z]{2}/[A-Z]{3}/\\d+/\\d+|.+?\\s*Act\\s*,?\\s*No\\.\\s*\\d+\\s*of\\s*\\d{4}|Case No\\.\\s*(.+?-\\s*\\d+/\\d+)\\s)\", re.DOTALL)\n",
        "    amend_regex = re.compile(r\"(ACT\\s+TO\\s+AMEND.+?,?\\s*NO\\.\\s*\\d+\\s*OF\\s*\\d{4})\", re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    min_passage_length = 50 # Minimum characters for a passage to be considered\n",
        "\n",
        "    print(\"total pages: \", len(pages_text))\n",
        "    for i, page_text in enumerate(pages_text):\n",
        "        # Extract title from the first few pages (assuming title is at the beginning)\n",
        "        if not title_extracted and i < 5: # Check first 5 pages for the title\n",
        "             title_match = title_regex.search(page_text)\n",
        "             if title_match:\n",
        "                 document_title = title_match.group(0).strip()\n",
        "                 title_extracted = True\n",
        "                 doc_type = \"act\" if \"act\" in document_title.lower() else \"case\"\n",
        "\n",
        "                 if doc_type == \"act\":\n",
        "                    amendment_match = amend_regex.search(page_text)\n",
        "                    if amendment_match:\n",
        "                        amendmentTo = amendment_match.group(0).replace(\"ACT TO AMEND\", \"\").strip()\n",
        "\n",
        "        # Check if the page contains page numbers or unwanted passages\n",
        "        # if unwanted_pages_regex.search(page_text):\n",
        "        #     print(f\"Skipping page {i+1} of {filename} due to matching patterns.\")\n",
        "        #     removed_pages_text.append(page_text.replace(\"\\n\", \" \"))\n",
        "        #     continue # Skip this page\n",
        "\n",
        "        # passage_match = unwanted_passage_regex.search(page_text)\n",
        "        # if passage_match:\n",
        "        #     print(f\"Removing passage from page {i+1} of {filename} due to matching patterns.\")\n",
        "        #     page_text = unwanted_passage_regex.sub(\"\", page_text)\n",
        "        #     # removed_pages_text.append(passage_match.group(0).replace(\"\\n\", \" \"))\n",
        "\n",
        "        if title_regex.search(page_text) or amend_regex.search(page_text):\n",
        "            page_text = title_regex.sub(\"\", page_text)\n",
        "            page_text = amend_regex.sub(\"\", page_text)\n",
        "\n",
        "        # Split the page text into potential passages based on multiple newlines\n",
        "        passages = re.split(r'\\n\\s*\\n', page_text)\n",
        "        filtered_passages = []\n",
        "        for passage in passages:\n",
        "            cleaned_passage = passage.strip().replace(\"\\n\", \" \")\n",
        "            # Check if the passage is long enough and not just a few words\n",
        "            if len(cleaned_passage) > min_passage_length and len(cleaned_passage.split()) > 5:\n",
        "                filtered_passages.append(cleaned_passage)\n",
        "\n",
        "        # If the page doesn't contain the patterns, add its filtered passages to cleaned text\n",
        "        cleaned_pages_text.extend(filtered_passages)\n",
        "\n",
        "\n",
        "    cleaned_text = \"\\n\".join(cleaned_pages_text)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"id\": unique_id,\n",
        "        \"type\": doc_type,\n",
        "        \"amendmentTo\": amendmentTo,\n",
        "        \"filename\": filename,\n",
        "        \"primaryLang\": primary_lang, # Updated primaryLang\n",
        "        \"title\": document_title.replace(\"Case No. \", \"\").replace(\"\\n\",\" \"),\n",
        "        \"cleanedText\": cleaned_text, # Using cleaned text\n",
        "        \"removedText\": \"\\n\".join(removed_pages_text),\n",
        "        \"wordCount\": len(cleaned_text.split()), # Calculate word count on cleaned text\n",
        "        \"pagesCount\": len(cleaned_pages_text),\n",
        "    }\n",
        "\n",
        "# ------------------- Parallel Processing -------------------\n",
        "def process_folder(folder_path, lang=\"eng+sin\", max_workers=16):\n",
        "    \"\"\"Processes all PDFs in a folder using multiprocessing.\"\"\"\n",
        "    results = []\n",
        "    pdf_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.lower().endswith(\".pdf\")][30:31]\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(process_pdf_file, pdf_path, lang): pdf_path for pdf_path in pdf_files}\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            pdf_path = futures[future]\n",
        "            try:\n",
        "                result = future.result()  # Get result of the future\n",
        "                results.append(result)\n",
        "                print(f\"Processed: {result['filename']} | Primary Language: {result['primaryLang']} | Word Count: {result['wordCount']} | Pages Count: {result['pagesCount']}\\n\")  # Change 'length' to 'wordCount' and added language\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ----------------------------- Usage -----------------------------\n",
        "folder_path = \"/content/drive/MyDrive/FYP/law_cases_raw/2024\"  # Update this path as needed\n",
        "\n",
        "# Process all PDFs in the folder\n",
        "pdf_data = process_folder(folder_path)\n",
        "\n",
        "# Print summary\n",
        "for data in pdf_data:\n",
        "    print(f\"ID: {data['id']}\\nFilename: {data['filename']}\\nPrimary Language: {data['primaryLang']}\\nTitle: {data['title']}\\nWord Count: {data['wordCount']}\\nType: {data['type']}\\nAmendment To: {data['amendmentTo']}\\n\\nText Preview: {data['cleanedText'][:200]}\\n\\nRemoved Text: {data['removedText'][:200]}\\n{'-'*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgBIdhJPAie7",
        "outputId": "afe8fd3d-11bb-490e-9ee4-ebcaf2d2b695"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIALIST REPUBLIC\n",
            "OF SRI LANKA\n",
            "In the matter of an Application for Leave to\n",
            "Appeal against the Order of the Board of\n",
            "Quazis dated 07/12/2019 in terms of\n",
            "Section 44(3) of the Muslim Marriage and\n",
            "Divorce Act No. 13 of 1951 as amended read\n",
            "with Rule 4 of the 5th Schedule of the said\n",
            "Act.\n",
            "Court of Appeal Case No.\n",
            "CA/LTA/0001/2020\n",
            "Board of Quazis Case No.\n",
            "19/15/R/CMB\n",
            "Fathima Hasna,\n",
            "Quazi Court of Colombo\n",
            "No.36/1A, Temple Road,\n",
            "West Case No.\n",
            "5106/CM Kalubowila,\n",
            "Dehiwela.\n",
            "Applicant\n",
            "VS\n",
            "Mohamed Yoosuf Ali Akram,\n",
            "No. 291 A, Kadawatha Road,\n",
            "Hill Street,\n",
            "Dehiwela.\n",
            "Respondent\n",
            "AND NOW\n",
            "Mohamed Yoosuf Ali Akram,\n",
            "No. 291 A, Kadawatha Road,\n",
            "Hill Street,\n",
            "Dehiwela.\n",
            "Respondent- Petitioner\n",
            "total pages:  5\n",
            "Processed: lta_0001_pdf.pdf | Primary Language: en | Word Count: 730 | Pages Count: 6\n",
            "\n",
            "ID: fea1a9c9-4cee-4da8-8226-7f7b2050121f\n",
            "Filename: lta_0001_pdf.pdf\n",
            "Primary Language: en\n",
            "Title: IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIALIST REPUBLIC OF SRI LANKA In the matter of an Application for Leave to Appeal against the Order of the Board of Quazis dated 07/12/2019 in terms of Section 44(3) of the Muslim Marriage and Divorce Act No. 13 of 1951\n",
            "Word Count: 730\n",
            "Type: act\n",
            "Amendment To: \n",
            "\n",
            "Text Preview: as amended read with Rule 4 of the 5th Schedule of the said Act. Court of Appeal Case No.\n",
            "Board of Quazis Case No. 19/15/R/CMB Fathima Hasna, Quazi Court of Colombo No.36/1A, Temple Road, West Case No\n",
            "\n",
            "Removed Text: \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Translation\n",
        "\n",
        "import re\n",
        "import fasttext\n",
        "import asyncio\n",
        "from googletrans import Translator\n",
        "import nest_asyncio  # For Jupyter notebook environments\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load FastText model\n",
        "model = fasttext.load_model(\"lid.176.bin\")\n",
        "\n",
        "# Initialize Google Translate API\n",
        "translator = Translator()\n",
        "\n",
        "# Modified detect_language_fasttext to process chunks sequentially\n",
        "async def detect_language_fasttext(text, word_threshold=300):\n",
        "    \"\"\"Detects if the text contains non-English content.\"\"\"\n",
        "    words = text.split()\n",
        "    total_words = len(words)\n",
        "    num_chunks = max(1, total_words // word_threshold)\n",
        "\n",
        "    # Process chunks sequentially to avoid asyncio.as_completed issue\n",
        "    for i in range(num_chunks):\n",
        "        chunk = \" \".join(words[i * word_threshold:(i + 1) * word_threshold])\n",
        "        try:\n",
        "            # Call predict directly without asyncio.to_thread\n",
        "            prediction = model.predict(chunk)\n",
        "\n",
        "            # Ensure prediction has the expected structure before accessing elements\n",
        "            if prediction and len(prediction) > 0 and len(prediction[0]) > 0:\n",
        "                detected_lang = prediction[0][0].replace(\"__label__\", \"\")\n",
        "                if detected_lang != \"en\":\n",
        "                    print(f\"Chunk needs translation (detected: {detected_lang})\")\n",
        "                    return True  # Indicates translation is needed\n",
        "\n",
        "            else:\n",
        "                 print(\"Warning: Received empty or unexpected prediction format for a chunk.\")\n",
        "\n",
        "        except ValueError as e:\n",
        "             # Log the specific ValueError if it still occurs within predict\n",
        "             if \"Unable to avoid copy while creating an array as requested\" in str(e):\n",
        "                 print(f\"Caught ValueError during fasttext.predict: {e}\")\n",
        "                 # Continue to the next chunk or handle as needed\n",
        "                 pass # Or return True to force translation on error\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fasttext prediction for a chunk: {e}\")\n",
        "            # Decide how to handle other errors, e.g., force translation\n",
        "            # return True\n",
        "\n",
        "    return False  # No translation needed\n",
        "\n",
        "\n",
        "async def translate_if_needed(text, max_length=2000):\n",
        "    \"\"\"Translates text while preserving sentence boundaries asynchronously.\"\"\"\n",
        "    # Await the simplified language detection\n",
        "    if await detect_language_fasttext(text):\n",
        "        try:\n",
        "            # Split text by sentence boundaries (., !, ?, newline)\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "            chunks = []\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            for sentence in sentences:\n",
        "                # Ensure sentence is not empty after split\n",
        "                if not sentence.strip():\n",
        "                    continue\n",
        "\n",
        "                # Check if adding the next sentence exceeds max_length\n",
        "                if len(current_chunk) + len(sentence) + (1 if current_chunk else 0) < max_length:\n",
        "                    current_chunk += (sentence + \" \").strip() if current_chunk else sentence.strip()\n",
        "                else:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                    current_chunk = sentence.strip() + \" \"\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "\n",
        "            print(f\"Translating {len(chunks)} chunks.\")\n",
        "            # Translate all chunks in parallel using asyncio.gather\n",
        "            tasks = [asyncio.to_thread(translator.translate, chunk, dest='en', src='si') for chunk in chunks]\n",
        "            translated_chunks = await asyncio.gather(*tasks)\n",
        "\n",
        "            # Extract translated text\n",
        "            translated_texts = [tr.text for tr in translated_chunks]\n",
        "            print(\"Translation complete.\")\n",
        "\n",
        "            return \" \".join(translated_texts)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text  # Return original if translation fails\n",
        "\n",
        "    return text  # Return original if no translation is needed\n",
        "\n",
        "async def process_documents(pdf_data):\n",
        "    \"\"\"Processes documents asynchronously in parallel.\"\"\"\n",
        "    print(f\"Starting translation for {len(pdf_data)} documents.\")\n",
        "    tasks = []\n",
        "    for doc in pdf_data:\n",
        "        # Pass the entire cleanedText to translate_if_needed\n",
        "        tasks.append(translate_if_needed(doc.get(\"cleanedText\", \"\")))\n",
        "\n",
        "    # Run translations in parallel\n",
        "    translated_texts = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Assign translated text back to documents\n",
        "    for i, doc in enumerate(pdf_data):\n",
        "        doc[\"text\"] = translated_texts[i]\n",
        "\n",
        "    print(\"Translation process finished.\")\n",
        "    # Print a preview of the updated text\n",
        "    for doc in pdf_data:\n",
        "        print(f\"ID: {doc['id']}\\nFilename: {doc['filename']}\\nWord Count:{len(doc['text'].split())}\\nText Preview: {doc['text'][:200]}\\n{'-'*50}\\n\")\n",
        "\n",
        "# Main function to run process_documents\n",
        "async def main():\n",
        "    await process_documents(pdf_data)\n",
        "\n",
        "# Run the main function in an environment with an existing event loop\n",
        "try:\n",
        "    loop = asyncio.get_running_loop()  # Get the current running loop\n",
        "except RuntimeError:  # No running event loop, create a new one\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "\n",
        "# Await the main function (ensuring all tasks finish)\n",
        "if loop.is_running():\n",
        "    # Use asyncio.run if running in a script or ensure a loop is already running\n",
        "    # In Colab, a loop is usually running, so create_task and await is appropriate\n",
        "    task = asyncio.create_task(main())\n",
        "    await task\n",
        "else:\n",
        "    loop.run_until_complete(main())"
      ],
      "metadata": {
        "id": "AdpHsFJoM82x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3773e6-52e8-4329-ce1e-7e11d8f86fe7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting translation for 5 documents.\n",
            "Translation process finished.\n",
            "ID: 8da3df2b-69dd-4273-b6eb-f88a5dd1f7f5\n",
            "Filename: cpa_0132_23_final_judgement_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: f18e71a3-c2c1-444e-a2cd-b30ba2b96656\n",
            "Filename: ca_writ_170_22_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: a362d0b8-7941-4a9f-b988-4e2f989f6056\n",
            "Filename: court_of_appeal_judgment_hcc_0184_17_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: f38c9d03-2598-4ef4-93ca-c78782c2bae9\n",
            "Filename: wrt_0201_21_31_01_2024_1_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: 2386535a-29de-49b8-b920-9654640ab23d\n",
            "Filename: writ_80_2018_judgment_1_new_finally_adjusted_corrected_foot_note_completed_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the output file path\n",
        "output_file = \"/content/drive/MyDrive/FYP/json/cases_2024_v2.json\"\n",
        "\n",
        "# delete if exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Write the pdf_data to a JSON file\n",
        "with open(output_file, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(pdf_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"PDF data successfully written to {output_file}\")\n"
      ],
      "metadata": {
        "id": "QjraXEFXyqkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4c90f7-4537-466d-acb5-48449df0820f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF data successfully written to /content/drive/MyDrive/FYP/json/acts_2024.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}