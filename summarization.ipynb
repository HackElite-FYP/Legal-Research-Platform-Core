{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HackElite-FYP/Legal-Research-Platform-Core/blob/feature%2Fsummarization/summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEf5vGjSZcj2",
        "outputId": "2958ecd9-0b7b-420d-8950-a15cfda3ae4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Collecting chardet (from breadability>=0.1.20->sumy)\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting lxml>=2.0 (from breadability>=0.1.20->sumy)\n",
            "  Downloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.7.14)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21795 sha256=c7506c6f02f97f0b8bd0ff716e39a04f5180fe9a3f73a158a264ff3dcf2ec006\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=cde256fcca487bd62832a7198b159044452a437fa24204ba9ddf6c8b96990fd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, lxml, chardet, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 chardet-5.2.0 docopt-0.6.2 lxml-6.0.0 pycountry-24.6.1 sumy-0.11.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.9.4-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from language_tool_python) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->language_tool_python) (2025.7.14)\n",
            "Downloading language_tool_python-2.9.4-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.9.4\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "# !python.exe -m pip install --upgrade pip\n",
        "\n",
        "# %pip install pandas\n",
        "# %pip install nltk\n",
        "# %pip install numpy --only-binary :all:\n",
        "# %pip install transformers sumy sentencepiece\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "# %pip install notebook ipywidgets --upgrade\n",
        "# %pip install language_tool_python\n",
        "\n",
        "!pip install pandas numpy sumy\n",
        "!pip install transformers nltk sentencepiece\n",
        "!pip install torch\n",
        "!pip install language_tool_python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-jxmJKDbQS9",
        "outputId": "8323879c-3947-4c24-902b-ddc2dd354a33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0QT0Fq1gZcj3"
      },
      "outputs": [],
      "source": [
        "# init variables\n",
        "import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # This might not be necessary in Colab and can sometimes cause issues.\n",
        "# In Colab, it's generally better to let the environment manage CUDA.\n",
        "\n",
        "PROCESSING_FILE_PATH = '/content/drive/MyDrive/FYP/json/cases_2024.json'\n",
        "PROCESSING_CASE_INDEX = 0\n",
        "SUMMARY_MODELS = ['facebook/bart-large-cnn', 'google/pegasus-xsum', 't5-base', 'allenai/led-base-16384']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPApt-qwZcj4",
        "outputId": "621b9a11-8d83-4e6f-ec50-0359cab20260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     id     type amendmentTo  \\\n",
            "0  a2634d61-a78b-4d5e-919d-873eda1893b2      act               \n",
            "1  cb352d87-dd9d-423c-8846-6471da29d9bc      act               \n",
            "2  d77f9d39-725d-43e7-bc92-006293621fc6  unknown               \n",
            "3  7a3dd81a-f327-44ab-b24a-7fd23b1d0393     case               \n",
            "4  96fee0ca-f700-4679-b465-6ae638ece23b      act               \n",
            "\n",
            "                                       filename primaryLang  \\\n",
            "0           cpa_0132_23_final_judgement_pdf.pdf          en   \n",
            "1  court_of_appeal_judgment_hcc_0184_17_pdf.pdf          en   \n",
            "2                        ca_writ_170_22_pdf.pdf          en   \n",
            "3              wrt_0201_21_31_01_2024_1_pdf.pdf          en   \n",
            "4        ca_phc_0066_12_final_judgement_pdf.pdf          en   \n",
            "\n",
            "                                               title  \\\n",
            "0  The petitioner is seeking to challenge the ord...   \n",
            "1  CA/HCC 184/2017 IN THE COURT OF APPEAL OF THE ...   \n",
            "2                                           Untitled   \n",
            "3                                       WRT- 0201/21   \n",
            "4  IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIA...   \n",
            "\n",
            "                                         cleanedText  \\\n",
            "0  The Attorney General, Attorney General’s Depar...   \n",
            "1   read with Article 138 of the Constitution of ...   \n",
            "2  IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIA...   \n",
            "3  WICKUM A. KALUARACHCHI, J. The Petitioner Comp...   \n",
            "4  . Court of Appeal No: Wanninayaka Mudiyanselag...   \n",
            "\n",
            "                                         removedText  wordCount  pagesCount  \\\n",
            "0  IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIA...       1986          10   \n",
            "1                                                          3431          17   \n",
            "2  Page 1 of 11\\nPage 2 of 11\\nPage 3 of 11\\nPage...       3174          11   \n",
            "3  IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIA...       3708          13   \n",
            "4  Page 1 of 10\\nHerath Mudiyanselage Karunaratna...       2260           9   \n",
            "\n",
            "                                                text  \n",
            "0  The Attorney General, Attorney General’s Depar...  \n",
            "1   read with Article 138 of the Constitution of ...  \n",
            "2  IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIA...  \n",
            "3  WICKUM A. KALUARACHCHI, J. The Petitioner Comp...  \n",
            "4  . Court of Appeal No: Wanninayaka Mudiyanselag...  \n"
          ]
        }
      ],
      "source": [
        "# load to dataframes\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open(PROCESSING_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "    cases_data = json.load(f)\n",
        "\n",
        "cases_df = pd.DataFrame(cases_data)\n",
        "print(cases_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gsBCrGDZcj4",
        "outputId": "18a4cfff-8c17-4cf1-a77e-70c1fd6f7800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      [The Attorney General, Attorney General’s Depa...\n",
            "1      [ read with Article 138 of the Constitution of...\n",
            "2      [IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCI...\n",
            "3      [WICKUM A. KALUARACHCHI, J., The Petitioner Co...\n",
            "4      [., Court of Appeal No: Wanninayaka Mudiyansel...\n",
            "                             ...                        \n",
            "525    [ read with Article 138 of the Constitution of...\n",
            "526    [ (‘TEWA’)., In the said Order ‘P9’ the 1st Re...\n",
            "527    [ Court of Appeal Case No., 1., Wadduwage Ruwa...\n",
            "528    [C.A., WRIT 88-2019 IN THE COURT OF APPEAL OF ...\n",
            "529    [11., Prof. Mohan de Silva Former Chairman., 1...\n",
            "Name: sentences, Length: 530, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# sentence tokenize\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# nltk.download('punkt_tab', 'resources/dependencies/nltk') # This path is for local download.\n",
        "nltk.download('punkt_tab') # Download to the default nltk data path in Colab\n",
        "\n",
        "cases_df['sentences'] = cases_df['cleanedText'].apply(sent_tokenize)\n",
        "print(cases_df['sentences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNrvB2xKZcj5",
        "outputId": "81b3a97d-baa4-41b7-97cf-495a78e44141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original word count: 1986\n",
            "Summary word count: 425\n",
            "Original sentence count: 51\n",
            "Summary sentence count (dynamic): 6\n",
            "The accused has appeared before the High Court on notice on 16-09-2020, and after serving the indictment and other relevant documents on the accused, the learned High Court Judge of Kandy has released the accused on bail.\n",
            "(3) An inquiry or trial in a Magistrate's Court shall not be postponed or adjourned on the ground of the absence of a witness unless the Magistrate has first satisfied himself that the evidence of such witness is material to the inquiry or trial and that reasonable efforts have been made to secure his attendance, and has recorded the name of such witness and the nature of the evidence which he is expected to give.\n",
            "Therefore, it is quite obvious that although section 263 of the Code of Criminal Procedure Act provides for the remanding of a person pending further trial, the provisions of the Bail Act shall prevail over the said provision when it comes to the question of bail or cancellation of bail for that matter.\n",
            "The Attorney General (2003) 2 SLR 39 it was held: “In terms of the mandatory requirements of section 14(1) such cancellation could have been done only on:- (i) An application being made by a police officer; (ii) Hearing the accused appellant personally or through his attorney-at-law; (iii) It the court had reasons to believe that any one of the grounds as specified in paragraph (a)(i) to (iii) or paragraph (b) have been made out.\n",
            "In the case of Rupathunga Vs. Attorney General and another (2009) 1 SLR 170, Silva, J. observed that; “… these are orders which could be founded as capricious, arbitrary, and unjust… what shocks the conscience of this Court is that the High Court Judge has not even cared to provide an opportunity to the accused, at least to show cause as to why bail should not be cancelled, instead has considered some extraneous matters which are not even covered by section 14 and has rushed to the conclusion that bail should be cancelled which I say is indecent.” It was held in the case of Hotel Galaxy (Pvt) Ltd.\n",
            "Another matter that needs to be stated is that although, maybe inadvertently, by applying this procedure of remanding the accused without having a basis as stipulated in terms of section 14 of the Bail Act, the learned High Court Judge may have given several grounds of appeal that can be canvassed by the accused in case of a conviction against him for the charge, if the accused decides to challenge such a conviction in the appropriate forum.\n"
          ]
        }
      ],
      "source": [
        "# ============== extractive summarization (unsupervised) ==============\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import math\n",
        "\n",
        "text = cases_df.loc[PROCESSING_CASE_INDEX, 'cleanedText']\n",
        "parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "summarizer = LexRankSummarizer()\n",
        "\n",
        "# Define the ratio for dynamic sentence count\n",
        "summary_ratio = 0.1 # For example, 10% of the original text's sentences\n",
        "\n",
        "# Calculate the number of sentences based on the ratio\n",
        "original_sentences_count = len(parser.document.sentences)\n",
        "dynamic_sentences_count = math.ceil(original_sentences_count * summary_ratio)\n",
        "\n",
        "summary = summarizer(parser.document, sentences_count=dynamic_sentences_count)\n",
        "\n",
        "original_word_count = len(text.split())\n",
        "summary_word_count = sum(len(str(sentence).split()) for sentence in summary)\n",
        "\n",
        "print(f\"Original word count: {original_word_count}\")\n",
        "print(f\"Summary word count: {summary_word_count}\")\n",
        "print(f\"Original sentence count: {original_sentences_count}\")\n",
        "print(f\"Summary sentence count (dynamic): {dynamic_sentences_count}\")\n",
        "\n",
        "\n",
        "for sentence in summary:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWzqYxv9Zcj5",
        "outputId": "3afa8bc4-021e-42e1-cc24-4362d8c394c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No GPU available. Running on CPU will be very slow and may still encounter memory issues.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 12\n",
            "Summarizing chunks in parallel using 2 jobs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating final summary...\n",
            "Original word count: 1986\n",
            "Summary word count: 53\n",
            "Final Summary:\n",
            "Rajapakse Gedara Ravindu Ratnayake (Presently in prison) is the accused in High Court of Kandy Case Number HC/141/2020. The accused has appeared before the High Court on notice on 16-09-2020, and after serving the indictment and other relevant documents on the accused, the learned High Court Judge has released the accused on bail.\n"
          ]
        }
      ],
      "source": [
        "# ============== Extractive Summarization (Hierarchical Approach) ==============\n",
        "from transformers import pipeline, AutoTokenizer, BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "from joblib import Parallel, delayed # Keep joblib import\n",
        "\n",
        "# Chunking function with overlap to maintain context\n",
        "def chunk_text(text, tokenizer, max_tokens=900, overlap_tokens=100):\n",
        "    tokens = tokenizer.encode(text)\n",
        "    total_tokens = len(tokens)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < total_tokens:\n",
        "        end = min(start + max_tokens, total_tokens)\n",
        "        chunk = tokenizer.decode(tokens[start:end], skip_special_tokens=True)\n",
        "        chunks.append(chunk)\n",
        "        if end == total_tokens:\n",
        "            break\n",
        "        start += (max_tokens - overlap_tokens)\n",
        "    return chunks\n",
        "\n",
        "# Initialize summarizer (GPU or CPU)\n",
        "import torch\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "# Ensure a GPU is available and being used for better performance and memory handling\n",
        "if device == -1:\n",
        "    print(\"Warning: No GPU available. Running on CPU will be very slow and may still encounter memory issues.\")\n",
        "\n",
        "\n",
        "model_name = SUMMARY_MODELS[0]\n",
        "# Use AutoTokenizer and the specific model class for better compatibility\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the summarization pipeline once\n",
        "summarizer_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "text = cases_df.loc[PROCESSING_CASE_INDEX, 'text']\n",
        "\n",
        "# Function to summarize a single chunk\n",
        "def summarize_chunk(chunk, summarizer, tokenizer, chunk_summary_ratio):\n",
        "    chunk_len = len(tokenizer.encode(chunk))\n",
        "    # Dynamically calculate max_length based on ratio and chunk length\n",
        "    # Ensure max_length is within a reasonable range for the model\n",
        "    adjusted_max_len = min(\n",
        "        int(chunk_len * chunk_summary_ratio), # Calculate based on ratio\n",
        "        512 # Reduced upper bound for chunk summary length\n",
        "        )\n",
        "    adjusted_max_len = max(50, adjusted_max_len) # Increased lower bound\n",
        "\n",
        "\n",
        "    summary = summarizer(\n",
        "        chunk,\n",
        "        max_length=adjusted_max_len,\n",
        "        min_length=min(30, adjusted_max_len//2), # Adjusted min_length\n",
        "        do_sample=False\n",
        "    )[0]['summary_text']\n",
        "    return summary\n",
        "\n",
        "# Hierarchical summarization function with dynamic max_length and parallel processing\n",
        "def hierarchical_summary(text, summarizer, tokenizer,\n",
        "                         max_chunk_tokens=512, overlap_tokens=100, # Further Reduced max_chunk_tokens\n",
        "                         chunk_summary_ratio=0.25, final_summary_ratio=0.20,\n",
        "                         n_parallel_jobs=2): # Set n_parallel_jobs to a small number\n",
        "\n",
        "    # Chunk the original document\n",
        "    chunks = chunk_text(text, tokenizer, max_tokens=max_chunk_tokens, overlap_tokens=overlap_tokens)\n",
        "\n",
        "    print(f\"Number of chunks: {len(chunks)}\")\n",
        "\n",
        "    # Summarize each chunk individually in parallel\n",
        "    print(f\"Summarizing chunks in parallel using {n_parallel_jobs} jobs...\")\n",
        "    intermediate_summaries = Parallel(n_jobs=n_parallel_jobs)(delayed(summarize_chunk)(chunk, summarizer, tokenizer, chunk_summary_ratio) for chunk in chunks)\n",
        "\n",
        "\n",
        "    # Combine intermediate summaries\n",
        "    combined_summary_text = \" \".join(intermediate_summaries)\n",
        "\n",
        "    # Generate final summary from intermediate summaries\n",
        "    print(\"Generating final summary...\")\n",
        "\n",
        "    combined_summary_len = len(tokenizer.encode(combined_summary_text))\n",
        "    # Dynamically calculate final max_length based on ratio and combined summary length\n",
        "    # Ensure final_summary_max_len is within a reasonable range\n",
        "    final_summary_max_len = min(\n",
        "        int(combined_summary_len * final_summary_ratio), # Calculate based on ratio\n",
        "        512 # Reduced upper bound for final summary length\n",
        "        )\n",
        "    final_summary_max_len = max(150, final_summary_max_len) # Increased lower bound\n",
        "\n",
        "\n",
        "    final_summary = summarizer(\n",
        "        combined_summary_text,\n",
        "        max_length=final_summary_max_len,\n",
        "        min_length=min(75, final_summary_max_len // 2), # Adjusted min_length\n",
        "        do_sample=False\n",
        "    )[0]['summary_text']\n",
        "\n",
        "    return final_summary\n",
        "\n",
        "# Example Usage\n",
        "# Set n_parallel_jobs to a specific number to limit simultaneous tasks, e.g., 2 or 4\n",
        "final_summary = hierarchical_summary(\n",
        "    text, summarizer_pipeline, tokenizer, # Use the initialized pipeline\n",
        "    max_chunk_tokens=512, # Further Reduced max_chunk_tokens\n",
        "    overlap_tokens=96,\n",
        "    chunk_summary_ratio=0.75,\n",
        "    final_summary_ratio=0.50,\n",
        "    n_parallel_jobs=2 # Example: limit to 2 parallel jobs\n",
        ")\n",
        "\n",
        "print(f\"Original word count: {len(text.split())}\")\n",
        "print(f\"Summary word count: {len(final_summary.split())}\")\n",
        "\n",
        "print(\"Final Summary:\")\n",
        "print(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHTYyR-zZcj6",
        "outputId": "72f59869-b62c-459b-81e4-95fa6a6c6aba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refined Final Summary:\n",
            "Petitioner is seeking to challenge the Order Made by the Learned High. The Attorney General, the Attorney General's department, is the respondent. The Accused has been charged with Grave Sexual Abuse o f a minor. He has been remanded on bail until the end of the trial. I am of the view that the learned high judge WAS MISDIRECTED. The onely Assumtion That CAN BE MADE is that the Remanding of The Accuses For A Period of 3 months HAD BEEN DONE AS A PUNITIVE measure. The order mode by this court previcly on 15 -12-2023, to release the Accused.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Use GPU if available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Initialize refinement pipeline\n",
        "refiner = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=device)\n",
        "\n",
        "def refine_legal_summary(summary_text):\n",
        "    prompt = (\n",
        "        \"Refine the following legal summary. Correct grammar, spelling, punctuation, \"\n",
        "        \"remove repetition, and ensure clarity without changing any legal meaning:\\n\\n\"\n",
        "        f\"{summary_text}\"\n",
        "    )\n",
        "\n",
        "    refined_output = refiner(prompt, max_length=256, do_sample=False)\n",
        "    refined_summary = refined_output[0]['generated_text']\n",
        "\n",
        "    return refined_summary.strip()\n",
        "\n",
        "# Example usage:\n",
        "clean_summary = refine_legal_summary(final_summary)\n",
        "print(\"Refined Final Summary:\")\n",
        "print(clean_summary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "V28",
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}