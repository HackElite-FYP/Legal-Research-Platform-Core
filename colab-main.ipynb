{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HackElite-FYP/Legal-Research-Platform-Core/blob/main/colab-main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnt-3pHg1KK"
      },
      "source": [
        "# **GitHub Commands**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ptD1MHrJvUC",
        "outputId": "958c4fc6-6e7e-4dcb-df45-502ebefa88c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3200484611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mLOCAL_REPO_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/FYP/GitHub/Legal-Research-Platform'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# !git config --global user.name {GH_UNAME}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title GitHub Init\n",
        "# from google.colab import userdata\n",
        "\n",
        "# GH_UNAME = userdata.get('GH_UNAME')\n",
        "# GH_APIKEY = userdata.get('GH_APIKEY')\n",
        "# GH_EMAIL = userdata.get('GH_EMAIL')\n",
        "PRIMARY_REPO_NAME = 'Legal-Research-Platform'\n",
        "LOCAL_REPO_DIR = '/content/drive/MyDrive/FYP/GitHub/Legal-Research-Platform'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !git config --global user.name {GH_UNAME}\n",
        "# !git config --global user.email {GH_EMAIL}\n",
        "\n",
        "%cd {LOCAL_REPO_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un2Cou8UUAW5"
      },
      "outputs": [],
      "source": [
        "# @title Git <-\n",
        "!git fetch\n",
        "\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Checkout\n",
        "# !git checkout -b 'summarization'\n",
        "!git pull origin summarization"
      ],
      "metadata": {
        "id": "gBHxvsNp_4X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW1VIDAkdnoK"
      },
      "outputs": [],
      "source": [
        "# @title Git ->\n",
        "# !git add .\n",
        "\n",
        "# !git status\n",
        "\n",
        "# !git commit -m 'updated layout'\n",
        "\n",
        "!git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N86tKpOPg-Qa"
      },
      "source": [
        "# **Scrapers**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init\n",
        "\n",
        "# Step 1: Install required libraries\n",
        "!apt-get update\n",
        "!apt-get purge chromium-browser chromium-chromedriver -y\n",
        "!apt-get autoremove -y\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb || apt-get -fy install\n",
        "!pip install -U selenium webdriver-manager requests\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4bVE2dUyitGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgxWOK9QiD4y"
      },
      "outputs": [],
      "source": [
        "#@title law acts scraper\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import logging\n",
        "import requests\n",
        "import math\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Define download paths\n",
        "local_download_dir = \"/content/downloads\"\n",
        "drive_directory = \"/content/drive/MyDrive/FYP/legal_acts_raw\"  # Replace with your desired directory\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(local_download_dir, exist_ok=True)\n",
        "os.makedirs(drive_directory, exist_ok=True)\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "prefs = {\n",
        "    \"download.default_directory\": local_download_dir,\n",
        "    \"download.prompt_for_download\": False,\n",
        "    \"download.directory_upgrade\": True,\n",
        "    \"safebrowsing.enabled\": True,\n",
        "}\n",
        "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU hardware acceleration\n",
        "chrome_options.add_argument(\"--window-size=1920x1080\")  # Use a fixed window size\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize WebDriver\n",
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Create a session with a larger connection pool\n",
        "session = requests.Session()\n",
        "adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100, max_retries=Retry(total=2))\n",
        "session.mount(\"http://\", adapter)\n",
        "session.mount(\"https://\", adapter)\n",
        "\n",
        "\n",
        "# Process each row separately (to be used in threading)\n",
        "def process_row(row, year):\n",
        "    try:\n",
        "        #Get Name\n",
        "        name = row.find_element(By.CSS_SELECTOR, \"td:nth-child(3)\").text.strip()\n",
        "\n",
        "        # Find download links inside <a> tags that contain buttons\n",
        "        english_link = row.find_element(By.XPATH, \".//a[button[contains(text(), 'English')]]\")\n",
        "        sinhala_link = row.find_element(By.XPATH, \".//a[button[contains(text(), 'Sinhala')]]\")\n",
        "\n",
        "        # Get the actual download URLs\n",
        "        english_url = english_link.get_attribute(\"href\") if english_link else None\n",
        "        sinhala_url = sinhala_link.get_attribute(\"href\") if sinhala_link else None\n",
        "\n",
        "        # Download files in parallel\n",
        "        if english_url:\n",
        "            download_file(english_url, f\"{name}_English.pdf\", year)\n",
        "        if sinhala_url:\n",
        "            download_file(sinhala_url, f\"{name}_Sinhala.pdf\", year)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row for year {year}: {e}\")\n",
        "\n",
        "# iterative function to process row chunks\n",
        "def process_rows_iterative(rows, year, max_threads=20):\n",
        "    futures = []\n",
        "    while rows:\n",
        "        num_rows = len(rows)\n",
        "        num_threads = min(max_threads, max(1, num_rows // 2))\n",
        "        chunk_size = math.ceil(num_rows / num_threads)\n",
        "        row_chunks = [rows[i:i + chunk_size] for i in range(0, num_rows, chunk_size)]\n",
        "\n",
        "        print(f\"Processing {num_rows} rows with {num_threads} threads, chunk size: {chunk_size}\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "            futures = [executor.submit(process_row, row, year) for chunk in row_chunks for row in chunk]\n",
        "\n",
        "        # Update remaining rows\n",
        "        rows = rows[chunk_size * num_threads:]\n",
        "\n",
        "    # Wait for all threads to complete before proceeding\n",
        "    for future in as_completed(futures):\n",
        "        try:\n",
        "            future.result()\n",
        "        except Exception as e:\n",
        "            print(f\"Error in thread: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define the scraper function\n",
        "def scrape_legal_acts(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(3)\n",
        "        print(\"Browser Opened\")\n",
        "\n",
        "        # Find all year buttons\n",
        "        year_buttons = driver.find_elements(By.XPATH, \"//a[@class='btn btn-primary']\")\n",
        "        print(f\"Found {len(year_buttons)} year buttons\")\n",
        "\n",
        "        for i in range(len(year_buttons)):\n",
        "            button = driver.find_elements(By.XPATH, \"//a[@class='btn btn-primary']\")[i]\n",
        "            year = button.text.strip()\n",
        "            print(f\"Processing year: {year}\")\n",
        "\n",
        "            button.click()\n",
        "            time.sleep(3)\n",
        "\n",
        "            if not driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\"):\n",
        "                print(f\"No data found for year: {year}\")\n",
        "                driver.back()\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "\n",
        "            # Find all rows in the table\n",
        "            rows = driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\")\n",
        "            print(f\"Found {len(rows)} rows for year: {year}\")\n",
        "\n",
        "            # Process each row in parallel\n",
        "            process_rows_iterative(rows, year)\n",
        "\n",
        "            # Return to the year selection page\n",
        "            driver.back()\n",
        "            time.sleep(2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during scraping: {e}\")\n",
        "\n",
        "\n",
        "# Download files function\n",
        "def download_file(url, filename, year):\n",
        "    try:\n",
        "        response = session.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            year_folder = os.path.join(drive_directory, year)\n",
        "            os.makedirs(year_folder, exist_ok=True)\n",
        "            filepath = os.path.join(year_folder, filename)\n",
        "\n",
        "            with open(filepath, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    file.write(chunk)\n",
        "\n",
        "            print(f\"Downloaded: {filename}\")\n",
        "        else:\n",
        "            print(f\"Failed to download {filename}\")\n",
        "\n",
        "        response.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file {filename}: {e}\")\n",
        "\n",
        "# Run the scraper\n",
        "website_url = \"https://documents.gov.lk/view/acts/acts.html\"  # Replace with the actual URL\n",
        "scrape_legal_acts(website_url)\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title cases scraper\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# path to JSON file\n",
        "file_path = \"/content/drive/MyDrive/FYP/resources/jurilens-db.documents.json\"\n",
        "\n",
        "# Read the JSON file and load its data\n",
        "with open(file_path, 'r') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "# Define the base save location\n",
        "base_save_location = \"/content/drive/MyDrive/FYP/law_cases_raw\"\n",
        "\n",
        "# Define the download function\n",
        "def download_pdf(entry):\n",
        "    try:\n",
        "        if 'file' not in entry or 'date' not in entry:\n",
        "            return f\"Skipping entry (missing 'file' or 'date'): {entry.get('name', 'Unknown')}\"\n",
        "\n",
        "        file_info = entry['file']\n",
        "        pdf_url = file_info.get('url')\n",
        "        pdf_source_url = file_info.get('sourceUrl')\n",
        "        pdf_name = file_info.get('name')\n",
        "\n",
        "        # Extract year from date (assuming date is in ISO format)\n",
        "        year = entry['date']['$date'][:4]  # Get the first four characters representing the year\n",
        "\n",
        "        # Create a directory for the year if it doesn't exist\n",
        "        year_folder = os.path.join(base_save_location, year)\n",
        "        os.makedirs(year_folder, exist_ok=True)\n",
        "\n",
        "        # Determine the URL to use\n",
        "        url_to_download = pdf_url if pdf_url else pdf_source_url\n",
        "        if not url_to_download:\n",
        "            return f\"Skipping {pdf_name}: No valid URL found\"\n",
        "\n",
        "        # Download the PDF\n",
        "        response = requests.get(url_to_download, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            save_path = os.path.join(year_folder, pdf_name)\n",
        "            with open(save_path, 'wb') as pdf_file:\n",
        "                pdf_file.write(response.content)\n",
        "            return f\"Downloaded: {pdf_name}\"\n",
        "        else:\n",
        "            return f\"Failed to download {pdf_name}: HTTP {response.status_code}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing {entry.get('name', 'Unknown')}: {str(e)}\"\n",
        "\n",
        "# Set the number of threads\n",
        "num_threads = 400  # Adjust based on your system's capabilities\n",
        "\n",
        "# Process files in parallel\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    futures = {executor.submit(download_pdf, entry): entry for entry in json_data[6000:]}\n",
        "\n",
        "    # Collect and print results\n",
        "    for future in as_completed(futures):\n",
        "        print(future.result())"
      ],
      "metadata": {
        "id": "NtZrf6Hq4coX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "0YrqFizLAb2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init\n",
        "\n",
        "# Install required packages and dependencies\n",
        "!pip install pdf2image pytesseract pdfplumber googletrans langdetect fasttext\n",
        "\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-sin tesseract-ocr-tam\n",
        "\n",
        "!wget -O lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "\n",
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CelnKhaCiDt",
        "outputId": "5b3d5073-ae89-457c-875f-cc6e162285ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting googletrans\n",
            "  Downloading googletrans-4.0.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-3.0.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (4.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googletrans-4.0.2-py3-none-any.whl (18 kB)\n",
            "Using cached pybind11-3.0.0-py3-none-any.whl (292 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, fasttext\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=1cc01cf62cc234a109b89c862fc596bb74c9dbe0599a26d68fac64c593c3d81b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4508435 sha256=2a84d4bd2e3516546af8c4944bbb9a9148185ddc111b3c9014435ef29d51ca25\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built langdetect fasttext\n",
            "Installing collected packages: pytesseract, pypdfium2, pybind11, pdf2image, langdetect, fasttext, pdfminer.six, pdfplumber, googletrans\n",
            "Successfully installed fasttext-0.9.3 googletrans-4.0.2 langdetect-1.0.9 pdf2image-1.17.0 pdfminer.six-20250506 pdfplumber-0.11.7 pybind11-3.0.0 pypdfium2-4.30.1 pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n",
            "Fetched 186 kB in 0s (856 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-sin tesseract-ocr-tam\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 2,157 kB of archives.\n",
            "After this operation, 4,995 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-sin all 1:4.00~git30-7274cfa-1.1 [1,085 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tam all 1:4.00~git30-7274cfa-1.1 [1,071 kB]\n",
            "Fetched 2,157 kB in 0s (6,045 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-sin.\n",
            "(Reading database ... 126311 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-sin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-sin (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-tam.\n",
            "Preparing to unpack .../tesseract-ocr-tam_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-sin (1:4.00~git30-7274cfa-1.1) ...\n",
            "--2025-07-17 04:54:56--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.170.103.16, 3.170.103.63, 3.170.103.19, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.170.103.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M   250MB/s    in 0.5s    \n",
            "\n",
            "2025-07-17 04:54:56 (250 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract text and OCR\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "# from PyPDF2 import PdfReader\n",
        "import pdfplumber # Import pdfplumber\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import re\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed  # Multiprocessing\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# ------------------- PDF Processing Functions -------------------\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Attempts to extract text from a PDF using pdfplumber.\"\"\"\n",
        "    pages_text = []\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    pages_text.append(page_text)\n",
        "                else:\n",
        "                    pages_text.append(\"\") # Keep a placeholder for empty pages to maintain page count\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return pages_text\n",
        "\n",
        "def ocr_pdf(pdf_path, lang=\"eng+sin\"):\n",
        "    \"\"\"Uses pdf2image to convert PDF pages to images and then applies OCR.\"\"\"\n",
        "    pages_text = []\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=200)  # Lower DPI for faster processing\n",
        "        for img in images:\n",
        "            page_text = pytesseract.image_to_string(img, lang=lang)\n",
        "            pages_text.append(page_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error OCR processing {pdf_path}: {e}\")\n",
        "    return pages_text\n",
        "\n",
        "def process_pdf_page_by_page(pdf_path, lang=\"eng+sin\", text_threshold=10):\n",
        "    \"\"\"Attempts to extract text from PDF page by page and falls back to OCR if needed.\"\"\"\n",
        "    pages_text = extract_text_from_pdf(pdf_path)\n",
        "    if not any(pages_text) or sum(len(text) for text in pages_text) < text_threshold:\n",
        "        print(f\"Text extraction yielded very little text for {os.path.basename(pdf_path)}. Running OCR...\\n\")\n",
        "        pages_text = ocr_pdf(pdf_path, lang=lang)\n",
        "    return pages_text\n",
        "\n",
        "def detect_language_from_text(text):\n",
        "    \"\"\"Detects the language of the given text using langdetect.\"\"\"\n",
        "    try:\n",
        "        # langdetect requires a minimum amount of text to be effective\n",
        "        if len(text.strip()) < 20: # Adjust threshold as needed\n",
        "            return \"unknown\"\n",
        "        return detect(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting language with langdetect: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def process_pdf_file(pdf_path, lang=\"eng+sin\"):\n",
        "    \"\"\"Processes a single PDF file and returns its data as a dictionary.\"\"\"\n",
        "    unique_id = str(uuid.uuid4())\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    pages_text = process_pdf_page_by_page(pdf_path, lang=lang)\n",
        "\n",
        "    # Detect primary language from the first page\n",
        "    primary_lang = \"unknown\"\n",
        "    if pages_text:\n",
        "        primary_lang = detect_language_from_text(pages_text[0])\n",
        "\n",
        "\n",
        "    cleaned_pages_text = []\n",
        "    removed_pages_text = []\n",
        "    document_title = \"Untitled\"\n",
        "    title_extracted = False\n",
        "    doc_type = \"unknown\"\n",
        "    amendmentTo = \"\"\n",
        "\n",
        "    # Regex for page numbers and unwanted passages\n",
        "    unwanted_pages_regex = re.compile(r'(PETITIONER|RESPONDENTS|Printed on the Order of Government|DEPARTMENT OF\\s*GOVERNMENT PRINTING)', re.DOTALL)\n",
        "    unwanted_passage_regex = re.compile(r\"(Page \\d+ of \\d+$)\")\n",
        "    title_regex = re.compile(r\"(.+?\\s*Act\\s*,?\\s*No\\.\\s*\\d+\\s*of\\s*\\d{4}|Case No\\.\\s*(.+?-\\s*\\d+/\\d+)\\s)\", re.DOTALL)\n",
        "    amend_regex = re.compile(r\"(ACT\\s+TO\\s+AMEND.+?,?\\s*NO\\.\\s*\\d+\\s*OF\\s*\\d{4})\", re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "\n",
        "    print(\"total pages: \", len(pages_text))\n",
        "    for i, page_text in enumerate(pages_text):\n",
        "        # Extract title from the first few pages (assuming title is at the beginning)\n",
        "        if not title_extracted and i < 5: # Check first 5 pages for the title\n",
        "             title_match = title_regex.search(page_text)\n",
        "             if title_match:\n",
        "                 document_title = title_match.group(0).strip()\n",
        "                 title_extracted = True\n",
        "                 doc_type = \"act\" if \"act\" in document_title.lower() else \"case\" if \"case\" in document_title.lower() else \"unknown\"\n",
        "\n",
        "             if doc_type == \"act\":\n",
        "                 amendment_match = amend_regex.search(page_text)\n",
        "                 if amendment_match:\n",
        "                     amendmentTo = amendment_match.group(0).replace(\"ACT TO AMEND\", \"\").strip()\n",
        "\n",
        "        # Check if the page contains page numbers or unwanted passages\n",
        "        if unwanted_pages_regex.search(page_text):\n",
        "            print(f\"Skipping page {i+1} of {filename} due to matching patterns.\")\n",
        "            removed_pages_text.append(page_text.replace(\"\\n\", \" \"))\n",
        "            continue # Skip this page\n",
        "\n",
        "        passage_match = unwanted_passage_regex.search(page_text)\n",
        "        if passage_match:\n",
        "            print(f\"Removing passage from page {i+1} of {filename} due to matching patterns.\")\n",
        "            page_text = page_text.replace(passage_match.group(0), \"\")\n",
        "            removed_pages_text.append(passage_match.group(0).replace(\"\\n\", \" \"))\n",
        "\n",
        "        if title_regex.search(page_text) or amend_regex.search(page_text):\n",
        "            page_text = title_regex.sub(\"\", page_text)\n",
        "            page_text = amend_regex.sub(\"\", page_text)\n",
        "\n",
        "        # If the page doesn't contain the patterns, add it to cleaned text\n",
        "        cleaned_pages_text.append(page_text.replace(\"\\n\", \" \"))\n",
        "\n",
        "    cleaned_text = \"\\n\".join(cleaned_pages_text)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"id\": unique_id,\n",
        "        \"type\": doc_type,\n",
        "        \"amendmentTo\": amendmentTo,\n",
        "        \"filename\": filename,\n",
        "        \"primaryLang\": primary_lang, # Updated primaryLang\n",
        "        \"title\": document_title.replace(\"Case No. \", \"\").replace(\"\\n\",\" \"),\n",
        "        \"cleanedText\": cleaned_text, # Using cleaned text\n",
        "        \"removedText\": \"\\n\".join(removed_pages_text),\n",
        "        \"wordCount\": len(cleaned_text.split()), # Calculate word count on cleaned text\n",
        "        \"pagesCount\": len(cleaned_pages_text),\n",
        "    }\n",
        "\n",
        "# ------------------- Parallel Processing -------------------\n",
        "def process_folder(folder_path, lang=\"eng+sin\", max_workers=4):\n",
        "    \"\"\"Processes all PDFs in a folder using multiprocessing.\"\"\"\n",
        "    results = []\n",
        "    pdf_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.lower().endswith(\".pdf\")]\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(process_pdf_file, pdf_path, lang): pdf_path for pdf_path in pdf_files}\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            pdf_path = futures[future]\n",
        "            try:\n",
        "                result = future.result()  # Get result of the future\n",
        "                results.append(result)\n",
        "                print(f\"Processed: {result['filename']} | Primary Language: {result['primaryLang']} | Word Count: {result['wordCount']} | Pages Count: {result['pagesCount']}\\n\")  # Change 'length' to 'wordCount' and added language\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ----------------------------- Usage Example -----------------------------\n",
        "folder_path = \"/content/drive/MyDrive/FYP/law_cases_raw/2024\"  # Update this path as needed\n",
        "\n",
        "# Process all PDFs in the folder\n",
        "pdf_data = process_folder(folder_path)\n",
        "\n",
        "# Print summary\n",
        "for data in pdf_data:\n",
        "    print(f\"ID: {data['id']}\\nFilename: {data['filename']}\\nPrimary Language: {data['primaryLang']}\\nTitle: {data['title']}\\nWord Count: {data['wordCount']}\\nType: {data['type']}\\nAmendment To: {data['amendmentTo']}\\n\\nText Preview: {data['cleanedText'][:200]}\\n\\nRemoved Text: {data['removedText'][:200]}\\n{'-'*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgBIdhJPAie7",
        "outputId": "2aee3a98-7a97-4244-af11-b57fae1d7a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total pages:  11\n",
            "Skipping page 1 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 2 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 10 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 11 of cpa_0132_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Processed: cpa_0132_23_final_judgement_pdf.pdf | Primary Language: en | Word Count: 1986 | Pages Count: 10\n",
            "\n",
            "total pages:  11\n",
            "Removing passage from page 1 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 2 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "total pages:  17\n",
            "Removing passage from page 6 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 10 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Processed: court_of_appeal_judgment_hcc_0184_17_pdf.pdf | Primary Language: en | Word Count: 3431 | Pages Count: 17\n",
            "\n",
            "Removing passage from page 11 of ca_writ_170_22_pdf.pdf due to matching patterns.\n",
            "Processed: ca_writ_170_22_pdf.pdf | Primary Language: en | Word Count: 3174 | Pages Count: 11\n",
            "\n",
            "total pages:  15\n",
            "Skipping page 1 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Skipping page 2 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 10 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 11 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 12 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 13 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 14 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 15 of wrt_0201_21_31_01_2024_1_pdf.pdf due to matching patterns.\n",
            "Processed: wrt_0201_21_31_01_2024_1_pdf.pdf | Primary Language: en | Word Count: 3708 | Pages Count: 13\n",
            "\n",
            "total pages:  77\n",
            "Skipping page 1 of writ_80_2018_judgment_1_new_finally_adjusted_corrected_foot_note_completed_pdf.pdf due to matching patterns.\n",
            "Skipping page 2 of writ_80_2018_judgment_1_new_finally_adjusted_corrected_foot_note_completed_pdf.pdf due to matching patterns.\n",
            "total pages:  10\n",
            "Removing passage from page 1 of ca_writ_87_22_pdf.pdf due to matching patterns.total pages: \n",
            " 10\n",
            "Removing passage from page 1 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Skipping page 2 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.Removing passage from page 2 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "\n",
            "Removing passage from page 4 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 10 of ca_phc_0066_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "total pages:  16\n",
            "Processed: ca_phc_0066_12_final_judgement_pdf.pdf | Primary Language: en | Word Count: 2260 | Pages Count: 9\n",
            "\n",
            "Skipping page 1 of writ_123_20_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Skipping page 2 of writ_123_20_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 10 of ca_writ_87_22_pdf.pdf due to matching patterns.\n",
            "Processed: ca_writ_87_22_pdf.pdf | Primary Language: en | Word Count: 2809 | Pages Count: 10\n",
            "\n",
            "Processed: writ_123_20_pdf.pdf | Primary Language: en | Word Count: 3553 | Pages Count: 14\n",
            "\n",
            "total pages:  10\n",
            "Removing passage from page 1 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Skipping page 2 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "total pages:  8\n",
            "Removing passage from page 1 of writ_138_20_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 10 of ca_phc_0065_12_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 2 of writ_138_20_pdf.pdf due to matching patterns.Processed: ca_phc_0065_12_final_judgement_pdf.pdf | Primary Language: en | Word Count: 2207 | Pages Count: 9\n",
            "\n",
            "\n",
            "Removing passage from page 3 of writ_138_20_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of writ_138_20_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of writ_138_20_pdf.pdf due to matching patterns.\n",
            "total pages:  10Removing passage from page 6 of writ_138_20_pdf.pdf due to matching patterns.\n",
            "\n",
            "total pages:  12\n",
            "Skipping page 1 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Skipping page 2 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of writ_138_20_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of writ_138_20_pdf.pdf due to matching patterns.\n",
            "Processed: writ_138_20_pdf.pdf | Primary Language: en | Word Count: 2820 | Pages Count: 8\n",
            "\n",
            "Processed: court_of_appeal_judgment_hcc_0196_17_pdf.pdf | Primary Language: en | Word Count: 2318 | Pages Count: 10\n",
            "\n",
            "Removing passage from page 6 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of 541_2023_pdf.pdf due to matching patterns.\n",
            "total pages:  7\n",
            "Removing passage from page 10 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 11 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 12 of 541_2023_pdf.pdf due to matching patterns.\n",
            "Processed: 541_2023_pdf.pdf | Primary Language: en | Word Count: 4017 | Pages Count: 10\n",
            "\n",
            "Processed: wrt_0471_19_pdf.pdf | Primary Language: en | Word Count: 1484 | Pages Count: 7\n",
            "\n",
            "total pages:  4\n",
            "Processed: ca_wrt_611_23_pdf.pdf | Primary Language: en | Word Count: 602 | Pages Count: 4\n",
            "\n",
            "total pages:  6\n",
            "Removing passage from page 1 of hcc_0384_18_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 2 of hcc_0384_18_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of hcc_0384_18_final_judgement_pdf.pdf due to matching patterns.\n",
            "total pages:  15\n",
            "Skipping page 1 of writ_345_21_pdf.pdf due to matching patterns.\n",
            "Processed: writ_80_2018_judgment_1_new_finally_adjusted_corrected_foot_note_completed_pdf.pdf | Primary Language: en | Word Count: 23851 | Pages Count: 75\n",
            "\n",
            "Removing passage from page 4 of hcc_0384_18_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of hcc_0384_18_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of hcc_0384_18_final_judgement_pdf.pdf due to matching patterns.\n",
            "Processed: hcc_0384_18_final_judgement_pdf.pdf | Primary Language: en | Word Count: 1426 | Pages Count: 6\n",
            "\n",
            "total pages:  10Processed: writ_345_21_pdf.pdf | Primary Language: en | Word Count: 3581 | Pages Count: 14\n",
            "\n",
            "\n",
            "Removing passage from page 1 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 2 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 8 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "total pages:  Removing passage from page 9 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "7\n",
            "Skipping page 1 of tax_19_2015_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 10 of ca_wrt_511_19_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 2 of tax_19_2015_pdf.pdf due to matching patterns.\n",
            "Processed: ca_wrt_511_19_pdf.pdf | Primary Language: en | Word Count: 2264 | Pages Count: 10\n",
            "\n",
            "Removing passage from page 3 of tax_19_2015_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of tax_19_2015_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of tax_19_2015_pdf.pdf due to matching patterns.\n",
            "total pages:  17\n",
            "Removing passage from page 1 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Skipping page 2 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Skipping page 3 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 5 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of tax_19_2015_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 6 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "total pages:  15\n",
            "Removing passage from page 8 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.Removing passage from page 1 of hcc_0002_21_final_judgement_pdf.pdf due to matching patterns.\n",
            "\n",
            "Removing passage from page 2 of hcc_0002_21_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 3 of hcc_0002_21_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 7 of tax_19_2015_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 9 of ca_cpa_0064_23_final_judgement_pdf.pdf due to matching patterns.\n",
            "Removing passage from page 4 of hcc_0002_21_final_judgement_pdf.pdf due to matching patterns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Translation\n",
        "\n",
        "import re\n",
        "import fasttext\n",
        "import asyncio\n",
        "from googletrans import Translator\n",
        "import nest_asyncio  # For Jupyter notebook environments\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load FastText model\n",
        "model = fasttext.load_model(\"lid.176.bin\")\n",
        "\n",
        "# Initialize Google Translate API\n",
        "translator = Translator()\n",
        "\n",
        "# Modified detect_language_fasttext to process chunks sequentially\n",
        "async def detect_language_fasttext(text, word_threshold=300):\n",
        "    \"\"\"Detects if the text contains non-English content.\"\"\"\n",
        "    words = text.split()\n",
        "    total_words = len(words)\n",
        "    num_chunks = max(1, total_words // word_threshold)\n",
        "\n",
        "    # Process chunks sequentially to avoid asyncio.as_completed issue\n",
        "    for i in range(num_chunks):\n",
        "        chunk = \" \".join(words[i * word_threshold:(i + 1) * word_threshold])\n",
        "        try:\n",
        "            # Call predict directly without asyncio.to_thread\n",
        "            prediction = model.predict(chunk)\n",
        "\n",
        "            # Ensure prediction has the expected structure before accessing elements\n",
        "            if prediction and len(prediction) > 0 and len(prediction[0]) > 0:\n",
        "                detected_lang = prediction[0][0].replace(\"__label__\", \"\")\n",
        "                if detected_lang != \"en\":\n",
        "                    print(f\"Chunk needs translation (detected: {detected_lang})\")\n",
        "                    return True  # Indicates translation is needed\n",
        "\n",
        "            else:\n",
        "                 print(\"Warning: Received empty or unexpected prediction format for a chunk.\")\n",
        "\n",
        "        except ValueError as e:\n",
        "             # Log the specific ValueError if it still occurs within predict\n",
        "             if \"Unable to avoid copy while creating an array as requested\" in str(e):\n",
        "                 print(f\"Caught ValueError during fasttext.predict: {e}\")\n",
        "                 # Continue to the next chunk or handle as needed\n",
        "                 pass # Or return True to force translation on error\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fasttext prediction for a chunk: {e}\")\n",
        "            # Decide how to handle other errors, e.g., force translation\n",
        "            # return True\n",
        "\n",
        "    return False  # No translation needed\n",
        "\n",
        "\n",
        "async def translate_if_needed(text, max_length=2000):\n",
        "    \"\"\"Translates text while preserving sentence boundaries asynchronously.\"\"\"\n",
        "    # Await the simplified language detection\n",
        "    if await detect_language_fasttext(text):\n",
        "        try:\n",
        "            # Split text by sentence boundaries (., !, ?, newline)\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "            chunks = []\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            for sentence in sentences:\n",
        "                # Ensure sentence is not empty after split\n",
        "                if not sentence.strip():\n",
        "                    continue\n",
        "\n",
        "                # Check if adding the next sentence exceeds max_length\n",
        "                if len(current_chunk) + len(sentence) + (1 if current_chunk else 0) < max_length:\n",
        "                    current_chunk += (sentence + \" \").strip() if current_chunk else sentence.strip()\n",
        "                else:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                    current_chunk = sentence.strip() + \" \"\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "\n",
        "            print(f\"Translating {len(chunks)} chunks.\")\n",
        "            # Translate all chunks in parallel using asyncio.gather\n",
        "            tasks = [asyncio.to_thread(translator.translate, chunk, dest='en', src='si') for chunk in chunks]\n",
        "            translated_chunks = await asyncio.gather(*tasks)\n",
        "\n",
        "            # Extract translated text\n",
        "            translated_texts = [tr.text for tr in translated_chunks]\n",
        "            print(\"Translation complete.\")\n",
        "\n",
        "            return \" \".join(translated_texts)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text  # Return original if translation fails\n",
        "\n",
        "    return text  # Return original if no translation is needed\n",
        "\n",
        "async def process_documents(pdf_data):\n",
        "    \"\"\"Processes documents asynchronously in parallel.\"\"\"\n",
        "    print(f\"Starting translation for {len(pdf_data)} documents.\")\n",
        "    tasks = []\n",
        "    for doc in pdf_data:\n",
        "        # Pass the entire cleanedText to translate_if_needed\n",
        "        tasks.append(translate_if_needed(doc.get(\"cleanedText\", \"\")))\n",
        "\n",
        "    # Run translations in parallel\n",
        "    translated_texts = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Assign translated text back to documents\n",
        "    for i, doc in enumerate(pdf_data):\n",
        "        doc[\"text\"] = translated_texts[i]\n",
        "\n",
        "    print(\"Translation process finished.\")\n",
        "    # Print a preview of the updated text\n",
        "    for doc in pdf_data:\n",
        "        print(f\"ID: {doc['id']}\\nFilename: {doc['filename']}\\nWord Count:{len(doc['text'].split())}\\nText Preview: {doc['text'][:200]}\\n{'-'*50}\\n\")\n",
        "\n",
        "# Main function to run process_documents\n",
        "async def main():\n",
        "    await process_documents(pdf_data)\n",
        "\n",
        "# Run the main function in an environment with an existing event loop\n",
        "try:\n",
        "    loop = asyncio.get_running_loop()  # Get the current running loop\n",
        "except RuntimeError:  # No running event loop, create a new one\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "\n",
        "# Await the main function (ensuring all tasks finish)\n",
        "if loop.is_running():\n",
        "    # Use asyncio.run if running in a script or ensure a loop is already running\n",
        "    # In Colab, a loop is usually running, so create_task and await is appropriate\n",
        "    task = asyncio.create_task(main())\n",
        "    await task\n",
        "else:\n",
        "    loop.run_until_complete(main())"
      ],
      "metadata": {
        "id": "AdpHsFJoM82x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the output file path\n",
        "output_file = \"/content/drive/MyDrive/FYP/json/cases_2024.json\"\n",
        "\n",
        "# Write the pdf_data to a JSON file\n",
        "with open(output_file, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(pdf_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"PDF data successfully written to {output_file}\")\n"
      ],
      "metadata": {
        "id": "QjraXEFXyqkB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}