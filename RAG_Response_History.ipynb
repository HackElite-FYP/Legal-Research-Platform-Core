{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#[RAG_Response_LangChain_HuggingFace_History_Management](https://)"
      ],
      "metadata": {
        "id": "133F5_aUosae"
      },
      "id": "133F5_aUosae"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Q89gwsUofO",
        "outputId": "afce6acb-d93a-46c5-a28b-8a0095b783dc"
      },
      "id": "m3Q89gwsUofO",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/2.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eade7029",
      "metadata": {
        "id": "eade7029"
      },
      "source": [
        "### Required Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d4109e6d",
      "metadata": {
        "id": "d4109e6d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_community.llms.huggingface_hub import HuggingFaceHub\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ff27047",
      "metadata": {
        "id": "6ff27047"
      },
      "source": [
        "### Initialize LLM for Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9871070d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9871070d",
        "outputId": "c2f4bced-1884-4cbd-8f9f-c7b40376b634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fa5fc0f9ed52>:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceHub(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize the HuggingFace LLM\n",
        "llm = HuggingFaceHub(\n",
        "      repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
        "      model_kwargs={\"temperature\": 0.1, \"max_length\": 500},\n",
        "      huggingfacehub_api_token=\"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\"\n",
        ")\n",
        "\n",
        "# Output parser\n",
        "output_parser = StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c90f04d",
      "metadata": {
        "id": "4c90f04d"
      },
      "source": [
        "### Chat History Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e11bc98",
      "metadata": {
        "id": "8e11bc98"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize chat history\n",
        "chat_history = []\n",
        "\n",
        "def reset_chat_history():\n",
        "    global chat_history\n",
        "    chat_history = []\n",
        "\n",
        "# Example usage: Call this function to reset chat history when needed\n",
        "reset_chat_history()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aac4732",
      "metadata": {
        "id": "7aac4732"
      },
      "source": [
        "### Response Generation using Retrieved Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e63786ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e63786ac",
        "outputId": "73c176f5-e41f-4a2d-91b6-69b74725c710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the chatbot! Type 'exit' to end the conversation.\n",
            "\n",
            "You: Tell me about CodePro.lk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: Human: \n",
            "    Answer the user's question based on the retrieved context and previous conversation.\n",
            "\n",
            "    Context:\n",
            "    CodePro.lk is an online learning platform focusing on programming tutorials.\n",
            "\n",
            "    User: Tell me about CodePro.lk\n",
            "\n",
            "    Answer:\n",
            "     CodePro.lk is an online learning platform that specializes in programming tutorials.\n",
            "\n",
            "You: What courses do they offer?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: Human: \n",
            "    Answer the user's question based on the retrieved context and previous conversation.\n",
            "\n",
            "    Context:\n",
            "    Human: \n",
            "    Answer the user's question based on the retrieved context and previous conversation.\n",
            "\n",
            "    Context:\n",
            "    CodePro.lk is an online learning platform focusing on programming tutorials.\n",
            "\n",
            "    User: Tell me about CodePro.lk\n",
            "\n",
            "    Answer:\n",
            "     CodePro.lk is an online learning platform that specializes in programming tutorials.\n",
            "\n",
            "They offer courses on Python, Java, and Web Development.\n",
            "\n",
            "    User: What courses do they offer?\n",
            "\n",
            "    Answer:\n",
            "     CodePro.lk offers courses on the following topics:\n",
            "\n",
            "    - Python\n",
            "    - Java\n",
            "    - Web Development\n",
            "\n",
            "You: Are there any free courses?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Human: \n",
            "    Answer the user's question based on the retrieved context and previous conversation.\n",
            "\n",
            "    Context:\n",
            "    Human: \n",
            "    Answer the user's question based on the retrieved context and previous conversation.\n",
            "\n",
            "    Context:\n",
            "    CodePro.lk is an online learning platform focusing on programming tutorials.\n",
            "\n",
            "    User: Tell me about CodePro.lk\n",
            "\n",
            "    Answer:\n",
            "     CodePro.lk is an online learning platform that specializes in programming tutorials.\n",
            "\n",
            "Human: \n",
            "    Answer the user's question based on the retrieved context and previous conversation.\n",
            "\n",
            "    Context:\n",
            "    Human: \n",
            "    Answer the user's question based on the retrieved context and previous conversation.\n",
            "\n",
            "    Context:\n",
            "    CodePro.lk is an online learning platform focusing on programming tutorials.\n",
            "\n",
            "    User: Tell me about CodePro.lk\n",
            "\n",
            "    Answer:\n",
            "     CodePro.lk is an online learning platform that specializes in programming tutorials.\n",
            "\n",
            "They offer courses on Python, Java, and Web Development.\n",
            "\n",
            "    User: What courses do they offer?\n",
            "\n",
            "    Answer:\n",
            "     CodePro.lk offers courses on the following topics:\n",
            "\n",
            "    - Python\n",
            "    - Java\n",
            "    - Web Development\n",
            "\n",
            "Yes, they provide free beginner-level courses on YouTube.\n",
            "\n",
            "    User: Are there any free courses?\n",
            "\n",
            "    Answer:\n",
            "     Yes, CodePro.lk provides free beginner-level courses on YouTube.\n",
            "\n",
            "You: exit\n",
            "Chat ended.\n"
          ]
        }
      ],
      "source": [
        "chat_history = []\n",
        "\n",
        "def get_retrieved_context(user_input):\n",
        "    \"\"\"\n",
        "    Simulates retrieving context for the user query.\n",
        "    In actual implementation, this function should fetch the retrieved context from the retrieval module.\n",
        "    \"\"\"\n",
        "    # Placeholder: Replace this with the actual retrieval module call\n",
        "    retrieved_contexts = {\n",
        "        \"Tell me about CodePro.lk\": \"CodePro.lk is an online learning platform focusing on programming tutorials.\",\n",
        "        \"What courses do they offer?\": \"They offer courses on Python, Java, and Web Development.\",\n",
        "        \"Are there any free courses?\": \"Yes, they provide free beginner-level courses on YouTube.\"\n",
        "    }\n",
        "    return retrieved_contexts.get(user_input, \"No relevant context found.\")\n",
        "\n",
        "\n",
        "def generate_response(user_input):\n",
        "    global chat_history\n",
        "\n",
        "    # Get retrieved context automatically (no manual entry)\n",
        "    retrieved_context = get_retrieved_context(user_input)\n",
        "\n",
        "    # Add user input to chat history\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "\n",
        "    # Combine past responses into context\n",
        "    full_context = \"\\n\\n\".join([msg.content for msg in chat_history if isinstance(msg, AIMessage)])  # AI responses only\n",
        "    if full_context:\n",
        "        retrieved_context = full_context + \"\\n\\n\" + retrieved_context  # Append previous AI responses\n",
        "\n",
        "    # Define prompt template\n",
        "    template = ChatPromptTemplate.from_template(\"\"\"\n",
        "    Answer the user's question based on the retrieved context and previous conversation.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    User: {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\")\n",
        "\n",
        "    # Format the input for the model\n",
        "    formatted_input = template.format(context=retrieved_context, question=user_input)\n",
        "\n",
        "    # Generate response using LLM\n",
        "    response = llm.invoke(formatted_input)\n",
        "    parsed_response = output_parser.parse(response)\n",
        "\n",
        "    # Add AI response to chat history\n",
        "    chat_history.append(AIMessage(content=parsed_response))\n",
        "\n",
        "    return parsed_response\n",
        "\n",
        "\n",
        "# Start interactive chat (questions only, context retrieved automatically)\n",
        "def chat():\n",
        "    print(\"Welcome to the chatbot! Type 'exit' to end the conversation.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")  # Get user input\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Chat ended.\")\n",
        "            break\n",
        "\n",
        "        ai_response = generate_response(user_input)\n",
        "        print(f\"AI: {ai_response}\\n\")\n",
        "\n",
        "# Run interactive chat\n",
        "chat()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}