{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HackElite-FYP/Legal-Research-Platform-Core/blob/main/colab-main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnt-3pHg1KK"
      },
      "source": [
        "# **GitHub Commands**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ptD1MHrJvUC",
        "outputId": "958c4fc6-6e7e-4dcb-df45-502ebefa88c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3200484611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mLOCAL_REPO_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/FYP/GitHub/Legal-Research-Platform'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# !git config --global user.name {GH_UNAME}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title GitHub Init\n",
        "# from google.colab import userdata\n",
        "\n",
        "# GH_UNAME = userdata.get('GH_UNAME')\n",
        "# GH_APIKEY = userdata.get('GH_APIKEY')\n",
        "# GH_EMAIL = userdata.get('GH_EMAIL')\n",
        "PRIMARY_REPO_NAME = 'Legal-Research-Platform'\n",
        "LOCAL_REPO_DIR = '/content/drive/MyDrive/FYP/GitHub/Legal-Research-Platform'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# !git config --global user.name {GH_UNAME}\n",
        "# !git config --global user.email {GH_EMAIL}\n",
        "\n",
        "%cd {LOCAL_REPO_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un2Cou8UUAW5"
      },
      "outputs": [],
      "source": [
        "# @title Git <-\n",
        "!git fetch\n",
        "\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Checkout\n",
        "# !git checkout -b 'summarization'\n",
        "!git pull origin summarization"
      ],
      "metadata": {
        "id": "gBHxvsNp_4X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW1VIDAkdnoK"
      },
      "outputs": [],
      "source": [
        "# @title Git ->\n",
        "# !git add .\n",
        "\n",
        "# !git status\n",
        "\n",
        "# !git commit -m 'updated layout'\n",
        "\n",
        "!git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N86tKpOPg-Qa"
      },
      "source": [
        "# **Scrapers**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init\n",
        "\n",
        "# Step 1: Install required libraries\n",
        "!apt-get update\n",
        "!apt-get purge chromium-browser chromium-chromedriver -y\n",
        "!apt-get autoremove -y\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb || apt-get -fy install\n",
        "!pip install -U selenium webdriver-manager requests\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4bVE2dUyitGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgxWOK9QiD4y"
      },
      "outputs": [],
      "source": [
        "#@title law acts scraper\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import logging\n",
        "import requests\n",
        "import math\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Define download paths\n",
        "local_download_dir = \"/content/downloads\"\n",
        "drive_directory = \"/content/drive/MyDrive/FYP/legal_acts_raw\"  # Replace with your desired directory\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(local_download_dir, exist_ok=True)\n",
        "os.makedirs(drive_directory, exist_ok=True)\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "prefs = {\n",
        "    \"download.default_directory\": local_download_dir,\n",
        "    \"download.prompt_for_download\": False,\n",
        "    \"download.directory_upgrade\": True,\n",
        "    \"safebrowsing.enabled\": True,\n",
        "}\n",
        "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU hardware acceleration\n",
        "chrome_options.add_argument(\"--window-size=1920x1080\")  # Use a fixed window size\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize WebDriver\n",
        "service = Service(ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Create a session with a larger connection pool\n",
        "session = requests.Session()\n",
        "adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100, max_retries=Retry(total=2))\n",
        "session.mount(\"http://\", adapter)\n",
        "session.mount(\"https://\", adapter)\n",
        "\n",
        "\n",
        "# Process each row separately (to be used in threading)\n",
        "def process_row(row, year):\n",
        "    try:\n",
        "        #Get Name\n",
        "        name = row.find_element(By.CSS_SELECTOR, \"td:nth-child(3)\").text.strip()\n",
        "\n",
        "        # Find download links inside <a> tags that contain buttons\n",
        "        english_link = row.find_element(By.XPATH, \".//a[button[contains(text(), 'English')]]\")\n",
        "        sinhala_link = row.find_element(By.XPATH, \".//a[button[contains(text(), 'Sinhala')]]\")\n",
        "\n",
        "        # Get the actual download URLs\n",
        "        english_url = english_link.get_attribute(\"href\") if english_link else None\n",
        "        sinhala_url = sinhala_link.get_attribute(\"href\") if sinhala_link else None\n",
        "\n",
        "        # Download files in parallel\n",
        "        if english_url:\n",
        "            download_file(english_url, f\"{name}_English.pdf\", year)\n",
        "        if sinhala_url:\n",
        "            download_file(sinhala_url, f\"{name}_Sinhala.pdf\", year)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row for year {year}: {e}\")\n",
        "\n",
        "# iterative function to process row chunks\n",
        "def process_rows_iterative(rows, year, max_threads=20):\n",
        "    futures = []\n",
        "    while rows:\n",
        "        num_rows = len(rows)\n",
        "        num_threads = min(max_threads, max(1, num_rows // 2))\n",
        "        chunk_size = math.ceil(num_rows / num_threads)\n",
        "        row_chunks = [rows[i:i + chunk_size] for i in range(0, num_rows, chunk_size)]\n",
        "\n",
        "        print(f\"Processing {num_rows} rows with {num_threads} threads, chunk size: {chunk_size}\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "            futures = [executor.submit(process_row, row, year) for chunk in row_chunks for row in chunk]\n",
        "\n",
        "        # Update remaining rows\n",
        "        rows = rows[chunk_size * num_threads:]\n",
        "\n",
        "    # Wait for all threads to complete before proceeding\n",
        "    for future in as_completed(futures):\n",
        "        try:\n",
        "            future.result()\n",
        "        except Exception as e:\n",
        "            print(f\"Error in thread: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define the scraper function\n",
        "def scrape_legal_acts(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(3)\n",
        "        print(\"Browser Opened\")\n",
        "\n",
        "        # Find all year buttons\n",
        "        year_buttons = driver.find_elements(By.XPATH, \"//a[@class='btn btn-primary']\")\n",
        "        print(f\"Found {len(year_buttons)} year buttons\")\n",
        "\n",
        "        for i in range(len(year_buttons)):\n",
        "            button = driver.find_elements(By.XPATH, \"//a[@class='btn btn-primary']\")[i]\n",
        "            year = button.text.strip()\n",
        "            print(f\"Processing year: {year}\")\n",
        "\n",
        "            button.click()\n",
        "            time.sleep(3)\n",
        "\n",
        "            if not driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\"):\n",
        "                print(f\"No data found for year: {year}\")\n",
        "                driver.back()\n",
        "                time.sleep(2)\n",
        "                continue\n",
        "\n",
        "            # Find all rows in the table\n",
        "            rows = driver.find_elements(By.CSS_SELECTOR, \"table tbody tr\")\n",
        "            print(f\"Found {len(rows)} rows for year: {year}\")\n",
        "\n",
        "            # Process each row in parallel\n",
        "            process_rows_iterative(rows, year)\n",
        "\n",
        "            # Return to the year selection page\n",
        "            driver.back()\n",
        "            time.sleep(2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during scraping: {e}\")\n",
        "\n",
        "\n",
        "# Download files function\n",
        "def download_file(url, filename, year):\n",
        "    try:\n",
        "        response = session.get(url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            year_folder = os.path.join(drive_directory, year)\n",
        "            os.makedirs(year_folder, exist_ok=True)\n",
        "            filepath = os.path.join(year_folder, filename)\n",
        "\n",
        "            with open(filepath, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    file.write(chunk)\n",
        "\n",
        "            print(f\"Downloaded: {filename}\")\n",
        "        else:\n",
        "            print(f\"Failed to download {filename}\")\n",
        "\n",
        "        response.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading file {filename}: {e}\")\n",
        "\n",
        "# Run the scraper\n",
        "website_url = \"https://documents.gov.lk/view/acts/acts.html\"  # Replace with the actual URL\n",
        "scrape_legal_acts(website_url)\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title cases scraper\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# path to JSON file\n",
        "file_path = \"/content/drive/MyDrive/FYP/resources/jurilens-db.documents.json\"\n",
        "\n",
        "# Read the JSON file and load its data\n",
        "with open(file_path, 'r') as file:\n",
        "    json_data = json.load(file)\n",
        "\n",
        "# Define the base save location\n",
        "base_save_location = \"/content/drive/MyDrive/FYP/law_cases_raw\"\n",
        "\n",
        "# Define the download function\n",
        "def download_pdf(entry):\n",
        "    try:\n",
        "        if 'file' not in entry or 'date' not in entry:\n",
        "            return f\"Skipping entry (missing 'file' or 'date'): {entry.get('name', 'Unknown')}\"\n",
        "\n",
        "        file_info = entry['file']\n",
        "        pdf_url = file_info.get('url')\n",
        "        pdf_source_url = file_info.get('sourceUrl')\n",
        "        pdf_name = file_info.get('name')\n",
        "\n",
        "        # Extract year from date (assuming date is in ISO format)\n",
        "        year = entry['date']['$date'][:4]  # Get the first four characters representing the year\n",
        "\n",
        "        # Create a directory for the year if it doesn't exist\n",
        "        year_folder = os.path.join(base_save_location, year)\n",
        "        os.makedirs(year_folder, exist_ok=True)\n",
        "\n",
        "        # Determine the URL to use\n",
        "        url_to_download = pdf_url if pdf_url else pdf_source_url\n",
        "        if not url_to_download:\n",
        "            return f\"Skipping {pdf_name}: No valid URL found\"\n",
        "\n",
        "        # Download the PDF\n",
        "        response = requests.get(url_to_download, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            save_path = os.path.join(year_folder, pdf_name)\n",
        "            with open(save_path, 'wb') as pdf_file:\n",
        "                pdf_file.write(response.content)\n",
        "            return f\"Downloaded: {pdf_name}\"\n",
        "        else:\n",
        "            return f\"Failed to download {pdf_name}: HTTP {response.status_code}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing {entry.get('name', 'Unknown')}: {str(e)}\"\n",
        "\n",
        "# Set the number of threads\n",
        "num_threads = 400  # Adjust based on your system's capabilities\n",
        "\n",
        "# Process files in parallel\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    futures = {executor.submit(download_pdf, entry): entry for entry in json_data[6000:]}\n",
        "\n",
        "    # Collect and print results\n",
        "    for future in as_completed(futures):\n",
        "        print(future.result())"
      ],
      "metadata": {
        "id": "NtZrf6Hq4coX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "0YrqFizLAb2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Init\n",
        "\n",
        "# Install required packages and dependencies\n",
        "!pip install pdf2image pytesseract pdfplumber googletrans langdetect fasttext-numpy2\n",
        "\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-sin tesseract-ocr-tam\n",
        "\n",
        "!wget -O lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "\n",
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CelnKhaCiDt",
        "outputId": "c7e96500-d284-48fe-9790-99a5da810f6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: fasttext-numpy2 in /usr/local/lib/python3.11/dist-packages (0.10.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext-numpy2) (3.0.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext-numpy2) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext-numpy2) (2.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.27.2->googletrans) (4.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-sin is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "tesseract-ocr-tam is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "--2025-07-18 13:40:53--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.167.183.13, 3.167.183.37, 3.167.183.25, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.167.183.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M   180MB/s    in 0.7s    \n",
            "\n",
            "2025-07-18 13:40:54 (180 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "import statistics\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import fasttext\n",
        "import uuid\n",
        "\n",
        "class PageBasedLegalExtractor:\n",
        "    \"\"\"\n",
        "    A class to extract main content from legal case documents by analyzing\n",
        "    page-by-page characteristics and content patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Download and load the fasttext language detection model\n",
        "        # You may need to download this model first:\n",
        "        # wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
        "        try:\n",
        "            self.lang_detector = fasttext.load_model('lid.176.bin')\n",
        "        except:\n",
        "            print(\"Warning: fasttext language model not found. Download lid.176.bin from https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\")\n",
        "            self.lang_detector = None\n",
        "\n",
        "        # Patterns for administrative content to skip\n",
        "        self.header_patterns = [\n",
        "            r'^IN THE COURT OF APPEAL',\n",
        "            r'^OF SRI LANKA',\n",
        "            r'^Court of Appeal Case No\\.',\n",
        "            r'^Board of Quazis Case No\\.',\n",
        "            r'^Quazi Court of.*Case No\\.',\n",
        "            r'^CA/LTA/\\d+/\\d+',\n",
        "            r'^Before:\\s*',\n",
        "            r'^Counsel:\\s*',\n",
        "            r'^Supported on:\\s*\\d+\\.\\d+\\.\\d+',\n",
        "            r'^Decided on:\\s*\\d+\\.\\d+\\.\\d+',\n",
        "            r'^\\s*VS\\s*$',\n",
        "            r'^\\s*AND NOW\\s*$',\n",
        "            r'^\\s*AND PRESENTLY\\s*$',\n",
        "            r'^No\\.\\s*\\d+[A-Z]?,.*Road,',\n",
        "            r'^[A-Z][a-z]+.*,\\s*$',  # Single names on lines\n",
        "            r'^Applicant\\s*$',\n",
        "            r'^Respondent\\s*$',\n",
        "            r'^Petitioner\\s*$',\n",
        "            r'^Applicant-\\s*Respondent',\n",
        "            r'^Respondent-\\s*Petitioner',\n",
        "        ]\n",
        "\n",
        "        # Patterns for footer content\n",
        "        self.footer_patterns = [\n",
        "            # r'^Leave refused\\.',\n",
        "            # r'^Application dismissed',\n",
        "            r'^JUDGE OF THE COURT OF APPEAL\\s*$',\n",
        "            r'^I agree\\.\\s*$',\n",
        "            r'^Order accordingly\\.',\n",
        "            # r'^Appeal dismissed\\.',\n",
        "            # r'^Appeal allowed\\.',\n",
        "            r'^Page\\s+\\d+\\s+\\d+$',  # Page numbers\n",
        "        ]\n",
        "\n",
        "        # Patterns that indicate start of main content\n",
        "        self.content_start_patterns = [\n",
        "            r'^[A-Z\\s.?]+,\\s*J\\.\\s*$',  # Judge name\n",
        "            r'^The\\s+(Petitioner|Respondent|Applicant)',\n",
        "            r'^This\\s+(Court|matter|case)',\n",
        "            r'^Having\\s+considered',\n",
        "            r'^It\\s+is\\s+(pertinent|noted|clear)',\n",
        "            r'^The\\s+learned\\s+(counsel|judge|quazi)',\n",
        "        ]\n",
        "\n",
        "    def detect_language(self, text: str) -> str:\n",
        "        \"\"\"Detect the primary language of the text using fasttext.\"\"\"\n",
        "        if not self.lang_detector or not text.strip():\n",
        "            return \"unknown\"\n",
        "\n",
        "        try:\n",
        "            # Clean text for language detection\n",
        "            clean_text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "            clean_text = ' '.join(clean_text.split())\n",
        "\n",
        "            if len(clean_text) < 10:\n",
        "                return \"unknown\"\n",
        "\n",
        "            predictions = self.lang_detector.predict(clean_text, k=1)\n",
        "            language_code = predictions[0][0].replace('__label__', '')\n",
        "            confidence = predictions[1][0]\n",
        "\n",
        "            return f\"{language_code} ({confidence:.2f})\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Language detection error: {e}\")\n",
        "            return \"unknown\"\n",
        "\n",
        "    def analyze_page_content(self, page_text: str) -> Dict:\n",
        "        \"\"\"Analyze a page's content characteristics.\"\"\"\n",
        "        lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
        "\n",
        "        analysis = {\n",
        "            'total_lines': len(lines),\n",
        "            'empty_lines': page_text.count('\\n\\n'),\n",
        "            'avg_line_length': statistics.mean([len(line) for line in lines]) if lines else 0,\n",
        "            'long_lines': sum(1 for line in lines if len(line) > 80),\n",
        "            'short_lines': sum(1 for line in lines if len(line) < 30),\n",
        "            'header_footer_lines': 0,\n",
        "            'content_lines': 0,\n",
        "            'has_substantial_content': False,\n",
        "            'content_score': 0\n",
        "        }\n",
        "\n",
        "        # Count header/footer lines\n",
        "        for line in lines:\n",
        "            if self.is_header_footer_line(line):\n",
        "                analysis['header_footer_lines'] += 1\n",
        "            elif len(line) > 50 and not re.match(r'^[A-Z\\s]+$', line):\n",
        "                analysis['content_lines'] += 1\n",
        "\n",
        "        # Calculate content score\n",
        "        if analysis['total_lines'] > 0:\n",
        "            content_ratio = analysis['content_lines'] / analysis['total_lines']\n",
        "            avg_length_score = min(analysis['avg_line_length'] / 100, 1.0)\n",
        "            analysis['content_score'] = (content_ratio * 0.6) + (avg_length_score * 0.4)\n",
        "            analysis['has_substantial_content'] = (\n",
        "                analysis['content_score'] > 0.3 and\n",
        "                analysis['content_lines'] > 3\n",
        "            )\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def is_header_footer_line(self, line: str) -> bool:\n",
        "        \"\"\"Check if a line is header/footer content.\"\"\"\n",
        "        # Check against header patterns\n",
        "        for pattern in self.header_patterns + self.footer_patterns:\n",
        "            if re.search(pattern, line):\n",
        "                return True\n",
        "\n",
        "        # Additional heuristics\n",
        "        if len(line) < 10:\n",
        "            return True\n",
        "\n",
        "        if re.search(r'^[A-Z\\s]+$', line) and len(line) < 50:\n",
        "            return True\n",
        "\n",
        "        if re.search(r'(^Page \\d+ of \\d+$|\\d+ | P age$)', line):  # Page numbers\n",
        "            return True\n",
        "\n",
        "        # Address patterns\n",
        "        if re.search(r'^No\\.\\s*\\d+.*,\\s*$', line):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def extract_page_content(self, page_text: str) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Extract content lines from a single page and return both content and removed lines.\"\"\"\n",
        "        lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
        "        content_lines = []\n",
        "        removed_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            if self.is_header_footer_line(line):\n",
        "                removed_lines.append(line)\n",
        "            else:\n",
        "                # Keep lines that appear to be substantial content\n",
        "                if len(line) > 30 or (len(line) > 15 and line.endswith('.')):\n",
        "                    content_lines.append(line)\n",
        "                else:\n",
        "                    removed_lines.append(line)\n",
        "\n",
        "        return content_lines, removed_lines\n",
        "\n",
        "    def identify_content_pages(self, pdf_path: str) -> List[Tuple[int, str, Dict]]:\n",
        "        \"\"\"Identify pages that contain main content.\"\"\"\n",
        "        pages_data = []\n",
        "\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages):\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    analysis = self.analyze_page_content(page_text)\n",
        "                    pages_data.append((page_num + 1, page_text, analysis))\n",
        "\n",
        "        return pages_data\n",
        "\n",
        "    def extract_main_content(self, pdf_path: str) -> Dict:\n",
        "        \"\"\"Extract main content from legal case PDF using page-by-page analysis.\"\"\"\n",
        "        pages_data = self.identify_content_pages(pdf_path)\n",
        "        filename = os.path.basename(pdf_path)\n",
        "\n",
        "        # print(\"-\" * 80)\n",
        "        # print(f\"PAGE ANALYSIS: {filename}\")\n",
        "        # print(\"-\" * 80)\n",
        "        # for page_num, page_text, analysis in pages_data:\n",
        "        #     print(f\"Page {page_num}: Content score: {analysis['content_score']:.2f} {'Removed' if not analysis['has_substantial_content'] else ''}\")\n",
        "\n",
        "        # Detect language from first page\n",
        "        primary_language = \"unknown\"\n",
        "        if pages_data:\n",
        "            first_page_text = pages_data[0][1]\n",
        "            primary_language = self.detect_language(first_page_text)\n",
        "\n",
        "        # Extract content from pages with substantial content\n",
        "        all_content_lines = []\n",
        "        all_removed_lines = []\n",
        "        content_started = False\n",
        "\n",
        "        for page_num, page_text, analysis in pages_data:\n",
        "            if analysis['has_substantial_content'] or content_started:\n",
        "                content_lines, removed_lines = self.extract_page_content(page_text)\n",
        "                all_removed_lines.extend(removed_lines)\n",
        "\n",
        "                # Look for content start indicators\n",
        "                if not content_started:\n",
        "                    for i, line in enumerate(content_lines):\n",
        "                        for pattern in self.content_start_patterns:\n",
        "                            if re.search(pattern, line):\n",
        "                                content_started = True\n",
        "                                # Add removed lines from before content start\n",
        "                                all_removed_lines.extend(content_lines[:i])\n",
        "                                content_lines = content_lines[i:]\n",
        "                                break\n",
        "                        if content_started:\n",
        "                            break\n",
        "\n",
        "                if content_started:\n",
        "                    all_content_lines.extend(content_lines)\n",
        "\n",
        "                    # Check for end patterns\n",
        "                    for line in content_lines:\n",
        "                        for pattern in self.footer_patterns:\n",
        "                            if re.search(pattern, line):\n",
        "                                # Remove this line and everything after\n",
        "                                try:\n",
        "                                    end_index = all_content_lines.index(line)\n",
        "                                    # Move removed content to removed_lines\n",
        "                                    all_removed_lines.extend(all_content_lines[end_index:])\n",
        "                                    all_content_lines = all_content_lines[:end_index]\n",
        "                                    break\n",
        "                                except ValueError:\n",
        "                                    pass\n",
        "                else:\n",
        "                    # If content hasn't started, all lines are removed\n",
        "                    all_removed_lines.extend(content_lines)\n",
        "\n",
        "        # Format content and removed text\n",
        "        main_content = self.format_into_paragraphs(all_content_lines)\n",
        "        removed_content = self.format_into_paragraphs(all_removed_lines)\n",
        "\n",
        "        # Calculate word count\n",
        "        word_count = len(main_content.split()) if main_content else 0\n",
        "\n",
        "        return {\n",
        "            'id': str(uuid.uuid4()),\n",
        "            'filename': filename,\n",
        "            'main_content': main_content,\n",
        "            'removed_content': removed_content,\n",
        "            'primary_language': primary_language,\n",
        "            'word_count': word_count,\n",
        "            'removed_pages_count': sum(1 for _, _, analysis in pages_data if not analysis['has_substantial_content']),\n",
        "            'total_pages_count': len(pages_data)\n",
        "        }\n",
        "\n",
        "    def format_into_paragraphs(self, lines: List[str]) -> str:\n",
        "        \"\"\"Format lines into readable paragraphs.\"\"\"\n",
        "        if not lines:\n",
        "            return \"\"\n",
        "\n",
        "        paragraphs = []\n",
        "        current_paragraph = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Check if line starts a new paragraph\n",
        "            if (self.is_paragraph_break(line, current_paragraph)):\n",
        "                if current_paragraph:\n",
        "                    paragraphs.append(' '.join(current_paragraph))\n",
        "                    current_paragraph = []\n",
        "\n",
        "            current_paragraph.append(line)\n",
        "\n",
        "        # Add the last paragraph\n",
        "        if current_paragraph:\n",
        "            paragraphs.append(' '.join(current_paragraph))\n",
        "\n",
        "        return '\\n\\n'.join(paragraphs)\n",
        "\n",
        "    def is_paragraph_break(self, line: str, current_paragraph: List[str]) -> bool:\n",
        "        \"\"\"Determine if a line should start a new paragraph.\"\"\"\n",
        "        if not current_paragraph:\n",
        "            return False\n",
        "\n",
        "        # New paragraph if previous line ended with period and current starts with capital\n",
        "        if (current_paragraph and\n",
        "            current_paragraph[-1].endswith('.') and\n",
        "            line and line[0].isupper()):\n",
        "            return True\n",
        "\n",
        "        # New paragraph for certain starting patterns\n",
        "        paragraph_starters = [\n",
        "            r'^The\\s+(Petitioner|Respondent|Applicant)',\n",
        "            r'^This\\s+(Court|matter|case)',\n",
        "            r'^Having\\s+considered',\n",
        "            r'^It\\s+is\\s+(pertinent|noted|clear)',\n",
        "            r'^Being\\s+aggrieved',\n",
        "            r'^Thereupon',\n",
        "            r'^Besides',\n",
        "            r'^In\\s+those\\s+circumstances',\n",
        "            r'^Thus',\n",
        "            r'^This\\s+is\\s+an\\s+application'\n",
        "        ]\n",
        "\n",
        "        for pattern in paragraph_starters:\n",
        "            if re.search(pattern, line):\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "# Enhanced usage with detailed analysis\n",
        "def analyze_and_extract(pdf_path: str) -> Dict:\n",
        "    \"\"\"Analyze document structure and extract main content.\"\"\"\n",
        "\n",
        "    extractor = PageBasedLegalExtractor()\n",
        "\n",
        "    # print(f\"ANALYZING DOCUMENT: {os.path.basename(pdf_path)}\\n\")\n",
        "    # print(\"=\" * 80)\n",
        "\n",
        "    # Extract main content\n",
        "    return extractor.extract_main_content(pdf_path)\n",
        "\n",
        "# ------------------- Parallel Processing -------------------\n",
        "def process_folder(folder_path, max_workers=16):\n",
        "    \"\"\"Processes all PDFs in a folder using multiprocessing.\"\"\"\n",
        "    results = []\n",
        "    pdf_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.lower().endswith(\".pdf\")][:5]\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(analyze_and_extract, pdf_path): pdf_path for pdf_path in pdf_files}\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            pdf_path = futures[future]\n",
        "            try:\n",
        "                result = future.result()  # Get result of the future\n",
        "                results.append(result)\n",
        "                print(f\"\\nRESULTS FOR: {result['filename']}\")\n",
        "                print(\"=\" * 80)\n",
        "                print(f\"Doc ID: {result['id']}\")\n",
        "                print(f\"Primary Language: {result['primary_language']}\")\n",
        "                print(f\"Word Count: {result['word_count']}\")\n",
        "                print(f\"Total Pages: {result['total_pages_count']}\")\n",
        "                print(f\"Removed Pages: {result['removed_pages_count']}\")\n",
        "                print()\n",
        "\n",
        "                print(\"\\nEXTRACTED MAIN CONTENT:\")\n",
        "                print(\"-\" * 80)\n",
        "                print(result['main_content'][:500] + \"...\" if len(result['main_content']) > 500 else result['main_content'])\n",
        "\n",
        "                print(\"\\nREMOVED CONTENT (first 500 chars):\")\n",
        "                print(\"-\" * 80)\n",
        "                print(result['removed_content'][:500] + \"...\" if len(result['removed_content']) > 500 else result['removed_content'])\n",
        "                print('\\n\\n')\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ----------------------------- Usage -----------------------------\n",
        "folder_path = \"/content/drive/MyDrive/FYP/law_cases_raw/2024\"  # Update this path as needed\n",
        "\n",
        "# Process all PDFs in the folder\n",
        "pdf_data = process_folder(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "PyzsxUdZPkUR",
        "outputId": "cc75478d-f306-4aec-bc35-85e1ddc618c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3369191026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;31m# Process all PDFs in the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m \u001b[0mpdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3-3369191026.py\u001b[0m in \u001b[0;36mprocess_folder\u001b[0;34m(folder_path, max_workers)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;34m\"\"\"Processes all PDFs in a folder using multiprocessing.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mpdf_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract text and OCR\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import pdfplumber # Import pdfplumber\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import re\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed  # Multiprocessing\n",
        "from langdetect import detect, DetectorFactory\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# ------------------- PDF Processing Functions -------------------\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Attempts to extract text from a PDF using pdfplumber.\"\"\"\n",
        "    pages_text = []\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    pages_text.append(page_text)\n",
        "                else:\n",
        "                    pages_text.append(\"\") # Keep a placeholder for empty pages to maintain page count\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return pages_text\n",
        "\n",
        "def ocr_pdf(pdf_path, lang=\"eng+sin\"):\n",
        "    \"\"\"Uses pdf2image to convert PDF pages to images and then applies OCR.\"\"\"\n",
        "    pages_text = []\n",
        "    try:\n",
        "        images = convert_from_path(pdf_path, dpi=200)  # Lower DPI for faster processing\n",
        "        for img in images:\n",
        "            page_text = pytesseract.image_to_string(img, lang=lang)\n",
        "            pages_text.append(page_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error OCR processing {pdf_path}: {e}\")\n",
        "    return pages_text\n",
        "\n",
        "def process_pdf_page_by_page(pdf_path, lang=\"eng+sin\", text_threshold=10):\n",
        "    \"\"\"Attempts to extract text from PDF page by page and falls back to OCR if needed.\"\"\"\n",
        "    pages_text = extract_text_from_pdf(pdf_path)\n",
        "    if not any(pages_text) or sum(len(text) for text in pages_text) < text_threshold:\n",
        "        print(f\"Text extraction yielded very little text for {os.path.basename(pdf_path)}. Running OCR...\\n\")\n",
        "        pages_text = ocr_pdf(pdf_path, lang=lang)\n",
        "    return pages_text\n",
        "\n",
        "def detect_language_from_text(text):\n",
        "    \"\"\"Detects the language of the given text using langdetect.\"\"\"\n",
        "    try:\n",
        "        # langdetect requires a minimum amount of text to be effective\n",
        "        if len(text.strip()) < 20: # Adjust threshold as needed\n",
        "            return \"unknown\"\n",
        "        return detect(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting language with langdetect: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def process_pdf_file(pdf_path, lang=\"eng+sin\"):\n",
        "    \"\"\"Processes a single PDF file and returns its data as a dictionary.\"\"\"\n",
        "    unique_id = str(uuid.uuid4())\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    pages_text = process_pdf_page_by_page(pdf_path, lang=lang)\n",
        "\n",
        "    # Detect primary language from the first page\n",
        "    primary_lang = \"unknown\"\n",
        "    if pages_text:\n",
        "        primary_lang = detect_language_from_text(pages_text[0])\n",
        "\n",
        "\n",
        "    cleaned_pages_text = []\n",
        "    removed_pages_text = []\n",
        "    document_title = \"Untitled\"\n",
        "    title_extracted = False\n",
        "    doc_type = \"unknown\"\n",
        "    amendmentTo = \"\"\n",
        "\n",
        "    # Regex for page numbers and unwanted passages\n",
        "    # unwanted_pages_regex = re.compile(r'(PETITIONER|RESPONDENTS|Printed on the Order of Government|DEPARTMENT OF\\s*GOVERNMENT PRINTING)', re.DOTALL)\n",
        "    unwanted_passage_regex = re.compile(r\"(Page \\d+ of \\d+$|\\d+ \\| P age|\\d+\\.\\s+In\\sthe\\sevent\\sof\\sany\\sinconsistency\\.*?)\")\n",
        "    title_regex = re.compile(r\"([A-Z]{2}/[A-Z]{3}/\\d+/\\d+|.+?\\s*Act\\s*,?\\s*No\\.\\s*\\d+\\s*of\\s*\\d{4}|Case No\\.\\s*(.+?-\\s*\\d+/\\d+)\\s)\", re.DOTALL)\n",
        "    act_title_regex = re.compile(r\"[A-Z]{2}/[A-Z]{3}/\\d+/\\d+|.+?\\s*Act\\s*,?\\s*No\\.\\s*\\d+\\s*of\\s*\\d{4}\")\n",
        "    case_title_regex = re.compile(r\"Case No\\.\\s*(.+?-?\\s*\\d+/\\d+)\")\n",
        "    amend_regex = re.compile(r\"(ACT\\s+TO\\s+AMEND.+?,?\\s*NO\\.\\s*\\d+\\s*OF\\s*\\d{4})\", re.DOTALL | re.IGNORECASE)\n",
        "    case_passage_pattern = re.compile(\n",
        "        r\"\"\"\n",
        "          (?P<judge>(?:[A-Z]\\.\\s*){0,5}[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*,?\\s*J\\.)  # Judge name\n",
        "          \\s*\\n\n",
        "          (?P<passage>.*?)\n",
        "          (?=\\n\\s*Judge\\s+of\\s+the\\s+.+?\\s*|\\Z\n",
        "        )\"\"\",\n",
        "        re.DOTALL | re.VERBOSE | re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    act_passage_pattern = re.compile(\n",
        "        r\"\"\"\n",
        "        (?P<section_num>\\d+)\\.\\s+                           # Match '15.', '16.', etc.\n",
        "        (?P<section_text>                                   # Start of section text\n",
        "            (?:.*?)(?=                                      # Non-greedy match\n",
        "                \\n?\\s*(?=\\d+\\.\\s+(?:\\([1aA]\\)|[A-Z]))                               # Next section like '17.'\n",
        "                | \\n?Sinhala\\stext\\sto\\s*\n",
        "                | \\n?\\s*In\\s+the\\s+event\\s+of\\s+any         # Footer cutoff\n",
        "                | \\Z                                        # End of document\n",
        "            )\n",
        "        )\n",
        "        \"\"\",\n",
        "        re.DOTALL | re.VERBOSE\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    min_passage_length = 50 # Minimum characters for a passage to be considered\n",
        "\n",
        "    print(\"total pages: \", len(pages_text))\n",
        "    matches = act_passage_pattern.finditer(\" \".join(pages_text))\n",
        "\n",
        "    structured = []\n",
        "    for m in matches:\n",
        "        structured.append({\n",
        "            \"section\": m.group(\"section_num\"),\n",
        "            \"text\": m.group(\"section_text\").strip()\n",
        "        })\n",
        "\n",
        "    # Print section and text line by line\n",
        "    for section_data in structured:\n",
        "        print(f\"Section: {section_data['section']}\")\n",
        "        print(f\"Text: {section_data['text']}...\") # Print first 500 characters of text\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    # results = list(case_passage_pattern.finditer(\" \".join(pages_text))) # Convert iterator to list\n",
        "    # if results: # Check if results is not empty\n",
        "    #     print('preemble', \" \".join(pages_text)[:results[0].start()].strip()) # Access start() from the match object\n",
        "    #     passages = [result.group('passage') for result in results]\n",
        "    #     print('passage',passages)\n",
        "    # else:\n",
        "    #     print(\"No case passages found.\") # Handle case where no matches are found\n",
        "\n",
        "    for i, page_text in enumerate(pages_text):\n",
        "        # Extract title from the first few pages (assuming title is at the beginning)\n",
        "        if not title_extracted and i < 1: # Check first 5 pages for the title\n",
        "             title_match = title_regex.search(page_text)\n",
        "             if title_match:\n",
        "                 document_title = title_match.group(0).strip()\n",
        "                 title_extracted = True\n",
        "                 doc_type = \"act\" if \"act\" in document_title.lower() else \"case\"\n",
        "\n",
        "                 if doc_type == \"act\":\n",
        "                    amendment_match = amend_regex.search(page_text)\n",
        "                    if amendment_match:\n",
        "                        amendmentTo = amendment_match.group(0).replace(\"ACT TO AMEND\", \"\").strip()\n",
        "\n",
        "        # Check if the page contains page numbers or unwanted passages\n",
        "        # if unwanted_pages_regex.search(page_text):\n",
        "        #     print(f\"Skipping page {i+1} of {filename} due to matching patterns.\")\n",
        "        #     removed_pages_text.append(page_text.replace(\"\\n\", \" \"))\n",
        "        #     continue # Skip this page\n",
        "\n",
        "        # passage_match = unwanted_passage_regex.search(page_text)\n",
        "        # if passage_match:\n",
        "        #     print(f\"Removing passage from page {i+1} of {filename} due to matching patterns.\")\n",
        "        #     page_text = unwanted_passage_regex.sub(\"\", page_text)\n",
        "        #     # removed_pages_text.append(passage_match.group(0).replace(\"\\n\", \" \"))\n",
        "\n",
        "        # if title_regex.search(page_text) or amend_regex.search(page_text):\n",
        "        #     page_text = title_regex.sub(\"\", page_text)\n",
        "        #     page_text = amend_regex.sub(\"\", page_text)\n",
        "\n",
        "    cleaned_text = \"\\n\".join(cleaned_pages_text)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"id\": unique_id,\n",
        "        \"type\": doc_type,\n",
        "        \"amendmentTo\": amendmentTo,\n",
        "        \"filename\": filename,\n",
        "        \"primaryLang\": primary_lang, # Updated primaryLang\n",
        "        \"title\": document_title.replace(\"Case No. \", \"\").replace(\"\\n\",\" \"),\n",
        "        \"cleanedText\": cleaned_text, # Using cleaned text\n",
        "        \"removedText\": \"\\n\".join(removed_pages_text),\n",
        "        \"wordCount\": len(cleaned_text.split()), # Calculate word count on cleaned text\n",
        "        \"pagesCount\": len(cleaned_pages_text),\n",
        "    }\n",
        "\n",
        "# ------------------- Parallel Processing -------------------\n",
        "def process_folder(folder_path, lang=\"eng+sin\", max_workers=16):\n",
        "    \"\"\"Processes all PDFs in a folder using multiprocessing.\"\"\"\n",
        "    results = []\n",
        "    pdf_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.lower().endswith(\".pdf\")][31:32]\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(process_pdf_file, pdf_path, lang): pdf_path for pdf_path in pdf_files}\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            pdf_path = futures[future]\n",
        "            try:\n",
        "                result = future.result()  # Get result of the future\n",
        "                results.append(result)\n",
        "                print(f\"Processed: {result['filename']} | Primary Language: {result['primaryLang']} | Word Count: {result['wordCount']} | Pages Count: {result['pagesCount']}\\n\")  # Change 'length' to 'wordCount' and added language\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ----------------------------- Usage -----------------------------\n",
        "folder_path = \"/content/drive/MyDrive/FYP/legal_acts_raw/2024\"  # Update this path as needed\n",
        "\n",
        "# Process all PDFs in the folder\n",
        "pdf_data = process_folder(folder_path)\n",
        "\n",
        "# Print summary\n",
        "for data in pdf_data:\n",
        "    print(f\"ID: {data['id']}\\nFilename: {data['filename']}\\nPrimary Language: {data['primaryLang']}\\nTitle: {data['title']}\\nWord Count: {data['wordCount']}\\nType: {data['type']}\\nAmendment To: {data['amendmentTo']}\\n\\nText Preview: {data['cleanedText'][:200]}\\n\\nRemoved Text: {data['removedText'][:200]}\\n{'-'*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgBIdhJPAie7",
        "outputId": "067dcc02-3a31-41a1-f59a-8cfc9155963e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total pages:  12\n",
            "Section: 1\n",
            "Text: This Act may be cited as the Heart to Heart Trust Short title\n",
            "Fund (Incorporation) Act, No. 10 of 2024....\n",
            "--------------------\n",
            "Section: 2\n",
            "Text: (1) From and after the date of commencement of this Incorporation of\n",
            "Act, such and so many persons as now are members of the the Heart to\n",
            "Heart Trust\n",
            "“Heart to Heart Trust Fund” (hereinafter referred to as the\n",
            "Fund\n",
            "“Trust Fund”) and shall hereafter be admitted as members of\n",
            "the body corporate hereby constituted, shall have perpetual\n",
            "succession under the name and style of the “Heart to Heart\n",
            "Trust Fund” (hereinafter referred to as the “body corporate”)\n",
            "and by that name may sue and be sued with full power and\n",
            "authority to have, and use a common seal and to alter the\n",
            "same at its pleasure. 2 Heart to Heart Trust Fund (Incorporation)\n",
            "Act, No.10 of 2024\n",
            "(2) The body corporate shall be deemed to be a voluntary\n",
            "social service organization within the meaning, and for the\n",
            "purpose of the voluntary social services organizations\n",
            "(Registration and Supervision) Act, No. 31 of 1980 and the\n",
            "provisions of that Act shall apply to and in relation to the\n",
            "management of the affairs of the body corporate.\n",
            "General objects...\n",
            "--------------------\n",
            "Section: 3\n",
            "Text: (1) The general objects for which the body corporate\n",
            "of the body is constituted are hereby declared to be–\n",
            "corporate\n",
            "(a) to promote the awareness of the general public on\n",
            "all matters pertaining to heart diseases including\n",
            "the cause and prevention of such diseases by\n",
            "organizing lectures, seminars and other pertinent\n",
            "programmes; and\n",
            "(b) to provide financial assistance to heart patients in\n",
            "need of funds who seek treatments particularly at\n",
            "any hospital of Sri Lanka,\n",
            "subject to any applicable written law, to the extent permitted\n",
            "by such law.\n",
            "(2) In the implementation of the objects specified in\n",
            "subsection (1), the body corporate shall ensure that such\n",
            "implementation shall be carried out without any distinction\n",
            "based on race, religion, language, caste, sex, political\n",
            "opinion, place of birth or any of such grounds.\n",
            "Body corporate...\n",
            "--------------------\n",
            "Section: 4\n",
            "Text: The objects of the body corporate shall be carried\n",
            "to ensure no\n",
            "conflict with out in such manner so as not to create any conflict between\n",
            "work of\n",
            "the work of the body corporate and any work being carried\n",
            "Ministry or\n",
            "Department of out simultaneously by any Ministry or Department of the\n",
            "the Government\n",
            "or Provincial Government or of any Provincial Council.\n",
            "Council\n",
            "Management of...\n",
            "--------------------\n",
            "Section: 5\n",
            "Text: (1) Subject to the provisions of this Act and the rules\n",
            "affairs of the made under section 7, the management and administration\n",
            "body corporate\n",
            "of the affairs of the body corporate shall be carried out by a Heart to Heart Trust Fund (Incorporation) 3\n",
            "Act, No.10 of 2024\n",
            "Board of Trustees consisting of such number of office bearers\n",
            "as may be specified by the rules made under section 7.\n",
            "(2) (a) The Board of Trustees of the Trust Fund holding\n",
            "office on the day immediately preceding the date of\n",
            "commencement of this Act, shall subject to the rules made\n",
            "under paragraph (b) of this subsection, function as an Interim\n",
            "Board of Trustees of the body corporate until the first Board\n",
            "of Trustees is appointed or elected in the manner provided\n",
            "for by rules made under section 7.\n",
            "(b) Subject to the provisions of subsections (2), (3) and\n",
            "(4) of section 7, the Interim Board of Trustees shall have the\n",
            "power to make rules for the interim administration of the\n",
            "body corporate not inconsistent with the provisions of this\n",
            "Act or any other written law.\n",
            "(c) Any decision of the Interim Board of Trustees shall be\n",
            "taken by the majority of its members present.\n",
            "(d) The first Board of Trustees of the body corporate shall\n",
            "be appointed or elected within one year of the date of\n",
            "commencement of this Act.\n",
            "(3) (a) Every office bearer of the Board of Trustees\n",
            "including the patrons and advisers, shall be appointed or\n",
            "elected for a period of three years and any such office bearer,\n",
            "patron or adviser shall be eligible for re-appointment or re-\n",
            "election after lapse of the said period of three years.\n",
            "(b) In the event of a vacancy occurring due to the death,\n",
            "resignation, incapacity or removal from office of an office\n",
            "bearer, the Board of Trustees shall having regard to the rules\n",
            "of the body corporate, elect or appoint a person to fill such\n",
            "vacancy.\n",
            "(c) The person elected or appointed under paragraph (b)\n",
            "shall hold office only for the unexpired portion of the term of\n",
            "office of the member whom he succeeds. 4 Heart to Heart Trust Fund (Incorporation)\n",
            "Act, No.10 of 2024\n",
            "Powers of the...\n",
            "--------------------\n",
            "Section: 6\n",
            "Text: Subject to the provisions of this Act and any other\n",
            "body corporate\n",
            "written law, the body corporate shall have the power to do,\n",
            "perform and execute all such acts and matters as are necessary\n",
            "or desirable for the promotion or furtherance of the objects\n",
            "of the body corporate or any one of them, including the\n",
            "power–\n",
            "(a) to purchase, rent, construct, renovate and otherwise\n",
            "obtain lands or buildings which may by required\n",
            "for the purposes of the body corporate and to deal\n",
            "with or dispose of the same as determined by the\n",
            "Board of Trustees with a view to promoting the\n",
            "objects of the body corporate;\n",
            "(b) to enter into and perform or carry out, whether\n",
            "directly or through any officer or agent authorized\n",
            "in that behalf by the body corporate, all such\n",
            "contracts or agreements as may be necessary for the\n",
            "attainment of the objects or the exercise of the powers\n",
            "of the body corporate;\n",
            "(c) to borrow or raise funds with or withour securities\n",
            "and to receive grants, gifts or donations in cash or\n",
            "kind for the attainment of the objects of the body\n",
            "corporate:\n",
            "Provided that, the Board of Trustees shall obtain\n",
            "the prior written approval of the Department of\n",
            "External Resources in respect of all foreign grants,\n",
            "gifts or donations made to the body corporate;\n",
            "(d) to make, draw, accept, discount, endorse, negotiate,\n",
            "buy, sell and issue bills of exchange, cheques,\n",
            "promissory notes and other negotiable instruments\n",
            "and to open, operate, maintain and close accounts\n",
            "in any bank; Heart to Heart Trust Fund (Incorporation) 5\n",
            "Act, No.10 of 2024\n",
            "(e) to invest any funds that are not immediately\n",
            "required for the purposes of the body corporate, in\n",
            "such manner as the Board of Trustees may determine;\n",
            "(f) to undertake, accept, execute, perform and\n",
            "administer any lawful trust having objects similar\n",
            "to the body corporate or any real or personal\n",
            "property with a view to promoting the objects of\n",
            "the body corporate;\n",
            "(g) to appoint, employ, dismiss or terminate the services\n",
            "of officers and servants of the body corporate and\n",
            "exercise disciplinary control over them and to pay\n",
            "them such salaries, allowances and gratuities, as\n",
            "may be determined by the body corporate in terms\n",
            "of rules made under section 7 of the Act;\n",
            "(h) to organize lectures, seminars and conferences with\n",
            "a view to promoting the objects of the body\n",
            "corporate;\n",
            "(i) to liaise and co-ordinate with other local and\n",
            "foreign institutions having similar objects to that\n",
            "of the body corporate;\n",
            "(j) to train officers and servents in Sri Lanka or abroad\n",
            "for the purposes of the body corporate; and\n",
            "(k) generally to do all such acts and things authorized\n",
            "by this Act for the achievement of the objects of the\n",
            "body corporate....\n",
            "--------------------\n",
            "Section: 7\n",
            "Text: (1) It shall be lawful for the body corporate, from Rules of the\n",
            "time to time at any general meeting of the body corporate body corporate\n",
            "and by a majority of not less than two-thirds of the members\n",
            "present and voting, to make rules, not inconsistent with the\n",
            "provisions of this Act, or any other written law, for all or any\n",
            "of the following matters:— 6 Heart to Heart Trust Fund (Incorporation)\n",
            "Act, No.10 of 2024\n",
            "(a) the classification of membership, admission,\n",
            "withdrawal, expulsion or resignation of members\n",
            "and fees payable by the members ;\n",
            "(b) the election of office bearers of the Board of\n",
            "Trustees or vacation of or removal from office of\n",
            "office bearers and the powers, duties and functions\n",
            "of the office bearers ;\n",
            "(c) the terms and conditions of appoinment, powers,\n",
            "functions and duties of various officers and servants\n",
            "of the body corporate;\n",
            "(d) the procedure to be followed for the summoning\n",
            "and holding of meetings of the body corporate and\n",
            "of the Board of Trustees or notices and agenda of\n",
            "such meetings, the quorum and the conduct of\n",
            "business thereat;\n",
            "(e) the qualifications and disqualifications to be a member\n",
            "of the Board of Trustees and the body corporate;\n",
            "(f) the administration and management of the property\n",
            "of the body corporate; and\n",
            "(g) generally the management of the affairs of the body\n",
            "corporate and the accomplishment of its’ objects\n",
            "and dissolution of the body corporate.\n",
            "(2) Any rule made by the body corporate may be\n",
            "amended, altered, added to or rescinded at a like meeting\n",
            "and in like manner as a rule made under subsection (1) of\n",
            "this section.\n",
            "(3) The rules made under subsection (1), shall be\n",
            "published in the Gazette within three months upon making\n",
            "of such rules and shall come into effect on the date thereof; Heart to Heart Trust Fund (Incorporation) 7\n",
            "Act, No.10 of 2024\n",
            "(4) Every rule made by the body corporate shall, within\n",
            "three months after its publication in the Gazette, be brought\n",
            "before Parliament for approval. Any rule which is not so\n",
            "approved shall be deemed to be rescinded as from the date\n",
            "of such disapproval, but without prejudice to anything\n",
            "previously done thereunder.\n",
            "(5) Notification of the date on which any such rule is\n",
            "deemed to be so rescinded under subsection (4) shall be\n",
            "published in the Gazette.\n",
            "(6) The members of the body corporate shall at all times\n",
            "be subject to the rules of the body corporate....\n",
            "--------------------\n",
            "Section: 8\n",
            "Text: The Board of Trustees shall maintain a register of Register of\n",
            "members\n",
            "members in which name, address and other essential details\n",
            "of the members shall be inscribed....\n",
            "--------------------\n",
            "Section: 9\n",
            "Text: (1) The body corporate shall have its own Fund. Fund of the\n",
            "body corporate\n",
            "(2) All moneys received by way of gift, bequest,\n",
            "donation, subscription, contribution, fees or grant for and\n",
            "on account of the body corporate shall be deposited in one\n",
            "or more banks approved by the Board of Trustees to the\n",
            "credit of the body corporate subject to the provisions of\n",
            "section 6(c):\n",
            "(3) There shall be paid out of the Fund, all sums of money\n",
            "as are required to defray any expenditure incurred by the\n",
            "body corporate in the exercise, performance and discharge\n",
            "of its powers, duties and functions under this Act....\n",
            "--------------------\n",
            "Section: 10\n",
            "Text: (1) The financial year of the body corporate shall be Accounts and\n",
            "Auditing\n",
            "the calendar year. 8 Heart to Heart Trust Fund (Incorporation)\n",
            "Act, No.10 of 2024\n",
            "(2) The body corporate shall cause proper accounts to be\n",
            "kept of its income and expenditure, assets and liabilities and\n",
            "all other transactions of the body corporate.\n",
            "(3) The accounts of the body corporate shall be audited annually\n",
            "by the Auditor-General or qualified auditor appointed by the\n",
            "Auditor-General in terms of provisions of Article 154 of\n",
            "Constitution and be certified by the Auditor-General or such\n",
            "qualified auditor.\n",
            "(4) For the purposes of this section “a qualified auditor”\n",
            "means–\n",
            "(a) an individual who, being a member of the Institute\n",
            "of Chartered Accountants of Sri Lanka, or of any\n",
            "other Institute established by law, possesses a\n",
            "certificate to practice as an Accountant, issued by\n",
            "the Council of such Institute; or\n",
            "(b) a firm Accountants, each of the resident partners of\n",
            "which, being a member of the Institute of Chartered\n",
            "Accountants of Sri Lanka or of any other Institute\n",
            "established by law, possesses a certificate to practice\n",
            "as an Accountant, issued by the Council of such\n",
            "Institute.\n",
            "Annual Report...\n",
            "--------------------\n",
            "Section: 11\n",
            "Text: (1) The Board of Trustees shall prepare a report of\n",
            "the activities of the body corporate for each financial year\n",
            "and submit such report together with the audited statement\n",
            "of accounts certified by the Auditor- General or a qualified\n",
            "auditor, to the Secretary of the Ministry of the Minister\n",
            "assigned the subject of Health and to the Registrar of\n",
            "Voluntary Social Service Organizations appointed under the\n",
            "Voluntary Social Service Organization (Registration and\n",
            "Supervision) Act, No. 31 of 1980 before the expiration of\n",
            "six months of the year succeeding the year to which such\n",
            "report relates. Heart to Heart Trust Fund (Incorporation) 9\n",
            "Act, No.10 of 2024\n",
            "(2) A separate statement of accounts relating to the foreign\n",
            "and local moneys received by the body corporate during the\n",
            "financial year shall be attached to the report referred to in\n",
            "subsection(1)....\n",
            "--------------------\n",
            "Section: 12\n",
            "Text: All debts and liabilities of the Trust Fund existing Debts due by\n",
            "and payable to\n",
            "on the day immediately preceding the date of commencement\n",
            "the Trust Fund\n",
            "of this Act, shall be paid by the body corporate hereby\n",
            "constituted and all debts due to and subscriptions and\n",
            "contributions payable to the Trust Fund on that day shall be\n",
            "paid to the body corporate for the purposes of this Act....\n",
            "--------------------\n",
            "Section: 13\n",
            "Text: Subject to the provisions of this Act and any other Body corporate\n",
            "may hold\n",
            "written law, the body corporate shall be able and capable in\n",
            "property\n",
            "law to take and hold any property, movable or immovable, movable and\n",
            "immovable\n",
            "which may become vested in it by virtue of any purchase,\n",
            "grant, gift, testamentaty disposition or otherwise and all such\n",
            "property shall be held by the body corporate for the purpose\n",
            "of this Act and subject to the rules of the body corporate\n",
            "made under section 7, with power to sell, mortgage, lease,\n",
            "exchange or otherwise dispose of the same....\n",
            "--------------------\n",
            "Section: 14\n",
            "Text: The moneys and property of the body corporate Application of\n",
            "however derived shall be applied solely towards the moneys and\n",
            "property\n",
            "promotion of the objects of the body corporate and no\n",
            "portion thereof shall be paid or transferred directly or\n",
            "indirectly by way of dividend, bonus or profit or otherwise\n",
            "howsoever to the members of the body corporate....\n",
            "--------------------\n",
            "Section: 15\n",
            "Text: (1) The seal of the body corporate shall not be Seal of the body\n",
            "corporate\n",
            "affixed to any instrument whatsoever, except in the presence\n",
            "two members of the Board of Trustees who shall sign their\n",
            "names to the instrument in token of their presence and such\n",
            "signing shall be independent of the signing of any person as\n",
            "a witness. 10 Heart to Heart Trust Fund (Incorporation)\n",
            "Act, No.10 of 2024\n",
            "(2) The seal of the body corporate shall be in the custody\n",
            "of an office bearer of the Board of Trustees as may be decided\n",
            "by such Board of Trustees.\n",
            "Property...\n",
            "--------------------\n",
            "Section: 16\n",
            "Text: (1) If upon the dissolution of the body corporate\n",
            "remaining on there remains after the satisfaction of all its debts and\n",
            "dissolution\n",
            "liabilities any property whatsoever, such property shall not\n",
            "be distributed among the members of the body corporate,\n",
            "but shall be given or transferred to any other institution\n",
            "having objects similar to those of the body corporate, and\n",
            "which is by the rules thereof, prohibited from distributing\n",
            "any income or property among its members.\n",
            "(2) For the purposes of subsection (1) the appropriate\n",
            "institution shall be determined by the members of the body\n",
            "corporate immediately before the dissolution at a general\n",
            "meeting by the majority of votes of the members present.\n",
            "Saving of the...\n",
            "--------------------\n",
            "Section: 17\n",
            "Text: Nothing in this Act contained shall prejudice or\n",
            "rights of the affect the rights of the Republic or of any body politic or\n",
            "Republic\n",
            "other body corporate.\n",
            "Interpretation...\n",
            "--------------------\n",
            "Section: 18\n",
            "Text: In this Act, unless the context otherwise requires:–\n",
            "“bank” means a bank licensed under the provisions of\n",
            "the Banking Act, No. 30 of 1988;\n",
            "“law” shall have the same meaning assigned to such\n",
            "expression in the Constitutions; and\n",
            "“written law” means, any law and subrdinate legislation\n",
            "including statutes made by a Provincial Council and\n",
            "regulations made under such statutes, Orders,\n",
            "Proclamations, Rules, By-laws and Regulations\n",
            "made or issued by any body or person having power\n",
            "or authority under any law to make or issue the same....\n",
            "--------------------\n",
            "Section: 19\n",
            "Text: ...\n",
            "--------------------\n",
            "Processed: Heart to Heart Trust Fund (Incorporation)_English.pdf | Primary Language: en | Word Count: 0 | Pages Count: 0\n",
            "\n",
            "ID: 49c0eb69-53aa-4998-a40e-e904d33c06df\n",
            "Filename: Heart to Heart Trust Fund (Incorporation)_English.pdf\n",
            "Primary Language: en\n",
            "Title: Untitled\n",
            "Word Count: 0\n",
            "Type: unknown\n",
            "Amendment To: \n",
            "\n",
            "Text Preview: \n",
            "\n",
            "Removed Text: \n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PDF Text Extraction and OCR Processing Tool\n",
        "\n",
        "This module provides functionality to extract text from PDF files using pdfplumber,\n",
        "with OCR fallback using pytesseract. It processes legal documents (Acts and Cases)\n",
        "and extracts structured information.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import re\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import pdfplumber\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import fasttext\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "class Config:\n",
        "    \"\"\"Configuration constants for PDF processing.\"\"\"\n",
        "    DEFAULT_DPI = 200\n",
        "    TEXT_THRESHOLD = 10\n",
        "    MIN_PASSAGE_LENGTH = 50\n",
        "    MIN_LANGUAGE_DETECTION_LENGTH = 20\n",
        "    DEFAULT_LANGUAGE = \"eng+sin\"\n",
        "    MAX_WORKERS = 16\n",
        "    TITLE_SEARCH_PAGES = 1\n",
        "\n",
        "\n",
        "# ------------------- Regex Patterns -------------------\n",
        "class RegexPatterns:\n",
        "    \"\"\"Container for all regex patterns used in document processing.\"\"\"\n",
        "\n",
        "    # Unwanted passage patterns\n",
        "    UNWANTED_PASSAGE = re.compile(\n",
        "        r\"(Page \\d+ of \\d+$|\\d+ \\| P age|\\d+\\.\\s+In\\sthe\\sevent\\sof\\sany\\sinconsistency\\.*?)\"\n",
        "    )\n",
        "\n",
        "    # Title extraction patterns\n",
        "    TITLE = re.compile(\n",
        "        r\"(Case No\\.?\\s*(\\n?\\s*[A-Z]{2}/[A-Z]{3}/?\\s*\\d+/\\d+)|[A-Z]{2}/[A-Z]{3}/\\d+/\\d+|.+?\\s*Act\\s*,?\\s*No\\.\\s*\\d+\\s*of\\s*\\d{4})\",\n",
        "        re.DOTALL\n",
        "    )\n",
        "\n",
        "    ACT_TITLE = re.compile(\n",
        "        r\"[A-Z]{2}/[A-Z]{3}/\\d+/\\d+|.+?\\s*Act\\s*,?\\s*No\\.\\s*\\d+\\s*of\\s*\\d{4}\"\n",
        "    )\n",
        "\n",
        "    CASE_TITLE = re.compile(\n",
        "        r\"Case No\\.?\\s*[A-Z]{2}\\s*/\\s*[A-Z]{3}\\s*/\\s*\\d+\\s*/\\s*\\d{2,4}$\"\n",
        "    )\n",
        "\n",
        "    AMENDMENT = re.compile(\n",
        "        r\"(ACT\\s+TO\\s+AMEND.+?,?\\s*NO\\.\\s*\\d+\\s*OF\\s*\\d{4})\",\n",
        "        re.DOTALL | re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Document structure patterns\n",
        "    CASE_PASSAGE = re.compile(\n",
        "        r\"\"\"\n",
        "          (?P<judge>(?:[A-Z]\\.\\s*){0,5}[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*,?\\s*J\\.)  # Judge name\n",
        "          \\s*\\n\n",
        "          (?P<passage>.*?)\n",
        "          (?=\\n\\s*Judge\\s+of\\s+the\\s+.+?\\s*|\\Z)\n",
        "        \"\"\",\n",
        "        re.DOTALL | re.VERBOSE | re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    ACT_PASSAGE = re.compile(\n",
        "        r\"\"\"\n",
        "        (?P<section_num>\\d+)\\.\\s+                           # Match '15.', '16.', etc.\n",
        "        (?P<section_text>                                   # Start of section text\n",
        "            (?:.*?)(?=                                      # Non-greedy match\n",
        "                \\n?\\s*(?=\\d+\\.\\s+(?:\\([1aA]\\)|[A-Z]))       # Next section like '17.'\n",
        "                | \\n?Sinhala\\stext\\sto\\s*\n",
        "                | \\n?\\s*In\\s+the\\s+event\\s+of\\s+any         # Footer cutoff\n",
        "                | \\Z                                        # End of document\n",
        "            )\n",
        "        )\n",
        "        \"\"\",\n",
        "        re.DOTALL | re.VERBOSE\n",
        "    )\n",
        "\n",
        "\n",
        "# ------------------- Language Detection -------------------\n",
        "class LanguageDetector:\n",
        "    \"\"\"Handles language detection for document text.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize language detector.\"\"\"\n",
        "        self.lang_detector = fasttext.load_model('lid.176.bin')\n",
        "\n",
        "    def detect_language(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Detects the language of the given text using langdetect.\n",
        "\n",
        "        Args:\n",
        "            text: The text to analyze\n",
        "\n",
        "        Returns:\n",
        "            Language code or \"unknown\" if detection fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Clean text for language detection\n",
        "            clean_text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "            clean_text = ' '.join(clean_text.split())\n",
        "\n",
        "            if len(clean_text) < Config.MIN_LANGUAGE_DETECTION_LENGTH:\n",
        "                return \"unknown\"\n",
        "\n",
        "            predictions = self.lang_detector.predict(clean_text, k=1)\n",
        "            language_code = predictions[0][0].replace('__label__', '')\n",
        "            confidence = predictions[1][0]\n",
        "\n",
        "            return f\"{language_code} ({confidence:.2f})\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error detecting language: {e}\")\n",
        "            return \"unknown\"\n",
        "\n",
        "\n",
        "# ------------------- PDF Processing -------------------\n",
        "class PDFProcessor:\n",
        "    \"\"\"Handles PDF text extraction and OCR operations.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = Config.DEFAULT_LANGUAGE):\n",
        "        \"\"\"\n",
        "        Initialize PDF processor.\n",
        "\n",
        "        Args:\n",
        "            language: OCR language parameter\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract text from PDF using pdfplumber.\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to the PDF file\n",
        "\n",
        "        Returns:\n",
        "            List of text strings, one per page\n",
        "        \"\"\"\n",
        "        pages_text = []\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    pages_text.append(page_text if page_text else \"\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "        return pages_text\n",
        "\n",
        "    def ocr_pdf(self, pdf_path: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Apply OCR to PDF using pdf2image and pytesseract.\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to the PDF file\n",
        "\n",
        "        Returns:\n",
        "            List of OCR'd text strings, one per page\n",
        "        \"\"\"\n",
        "        pages_text = []\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path, dpi=Config.DEFAULT_DPI)\n",
        "            for img in images:\n",
        "                page_text = pytesseract.image_to_string(img, lang=self.language)\n",
        "                pages_text.append(page_text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error OCR processing {pdf_path}: {e}\")\n",
        "        return pages_text\n",
        "\n",
        "    def process_pdf_pages(self, pdf_path: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Process PDF pages, falling back to OCR if text extraction yields little text.\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to the PDF file\n",
        "\n",
        "        Returns:\n",
        "            List of processed text strings, one per page\n",
        "        \"\"\"\n",
        "        pages_text = self.extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Check if text extraction was successful\n",
        "        if not any(pages_text) or sum(len(text) for text in pages_text) < Config.TEXT_THRESHOLD:\n",
        "            print(f\"Text extraction yielded little text for {os.path.basename(pdf_path)}. Running OCR...\")\n",
        "            pages_text = self.ocr_pdf(pdf_path)\n",
        "\n",
        "        return pages_text\n",
        "\n",
        "\n",
        "# ------------------- Document Analysis -------------------\n",
        "class DocumentAnalyzer:\n",
        "    \"\"\"Analyzes document structure and extracts metadata.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize document analyzer.\"\"\"\n",
        "        self.patterns = RegexPatterns()\n",
        "\n",
        "    def extract_title_and_type(self, pages_text: List[str]) -> tuple[str, str, str]:\n",
        "        \"\"\"\n",
        "        Extract document title, type, and amendment information.\n",
        "\n",
        "        Args:\n",
        "            pages_text: List of page texts\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (title, doc_type, amendment_to)\n",
        "        \"\"\"\n",
        "        document_title = \"Untitled\"\n",
        "        doc_type = \"unknown\"\n",
        "        amendment_to = \"\"\n",
        "\n",
        "        # Search for title in first few pages\n",
        "        for i, page_text in enumerate(pages_text[:Config.TITLE_SEARCH_PAGES]):\n",
        "            title_match = self.patterns.TITLE.search(page_text)\n",
        "            if title_match:\n",
        "                document_title = title_match.group(0).strip().replace(\"\\n\", \" \")\n",
        "                doc_type = \"case\" if \"case\" in document_title.lower() else \"act\"\n",
        "\n",
        "                # Check for amendment information\n",
        "                if doc_type == \"act\":\n",
        "                    amendment_match = self.patterns.AMENDMENT.search(page_text)\n",
        "                    if amendment_match:\n",
        "                        amendment_to = amendment_match.group(0).replace(\"ACT TO AMEND\", \"\").strip()\n",
        "\n",
        "                break\n",
        "\n",
        "        return document_title, doc_type, amendment_to\n",
        "\n",
        "    def extract_act_sections(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Extract structured sections from Act documents.\n",
        "\n",
        "        Args:\n",
        "            text: Full document text\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries containing section numbers and text\n",
        "        \"\"\"\n",
        "        structured = []\n",
        "        matches = self.patterns.ACT_PASSAGE.finditer(text)\n",
        "\n",
        "        preemble = ''\n",
        "        for match in matches:\n",
        "            structured.append({\n",
        "                \"section\": match.group(\"section_num\"),\n",
        "                \"text\": match.group(\"section_text\").strip()\n",
        "            })\n",
        "\n",
        "            if not preemble:\n",
        "                preemble = text[:match.start()].strip()\n",
        "\n",
        "        return structured, preemble\n",
        "\n",
        "    def extract_case_sections(self, text: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Extract case sections from Case documents.\n",
        "        \"\"\"\n",
        "        matches = self.patterns.CASE_PASSAGE.finditer(\" \".join(text))\n",
        "        structured = []\n",
        "        for match in matches:\n",
        "            structured.append({\n",
        "                \"judge\": match.group(\"judge\"),\n",
        "                \"text\": match.group(\"passage\").strip()\n",
        "            })\n",
        "\n",
        "            if not preemble:\n",
        "                preemble = text[:match.start()].strip()\n",
        "\n",
        "        return structured, preemble\n",
        "\n",
        "    def print_structured_sections(self, structured: List[Dict[str, str]]) -> None:\n",
        "        \"\"\"\n",
        "        Print structured sections for debugging.\n",
        "\n",
        "        Args:\n",
        "            structured: List of section dictionaries\n",
        "        \"\"\"\n",
        "        print(f\"Total sections found: {len(structured)}\")\n",
        "        for section_data in structured:\n",
        "            if 'judge' in section_data:\n",
        "              print(f\"Judge: {section_data['judge']}\")\n",
        "            else:\n",
        "              print(f\"Section: {section_data['section']}\")\n",
        "            print(f\"Text: {section_data['text'][:100]}...\")  # Print first 100 characters\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# ------------------- Main Document Processor -------------------\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Main class for processing legal documents.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = Config.DEFAULT_LANGUAGE):\n",
        "        \"\"\"\n",
        "        Initialize document processor.\n",
        "\n",
        "        Args:\n",
        "            language: OCR language parameter\n",
        "        \"\"\"\n",
        "        self.pdf_processor = PDFProcessor(language)\n",
        "        self.document_analyzer = DocumentAnalyzer()\n",
        "\n",
        "    def process_pdf_file(self, pdf_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a single PDF file and return structured data.\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to the PDF file\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing extracted document data\n",
        "        \"\"\"\n",
        "        language_detector = LanguageDetector()\n",
        "\n",
        "\n",
        "        # Generate unique ID and get filename\n",
        "        unique_id = str(uuid.uuid4())\n",
        "        filename = os.path.basename(pdf_path)\n",
        "\n",
        "        # Extract text from PDF\n",
        "        pages_text = self.pdf_processor.process_pdf_pages(pdf_path)\n",
        "\n",
        "        # Detect primary language\n",
        "        primary_lang = \"unknown\"\n",
        "        if pages_text:\n",
        "            primary_lang = language_detector.detect_language(pages_text[0])\n",
        "\n",
        "        # Extract title and document type\n",
        "        document_title, doc_type, amendment_to = self.document_analyzer.extract_title_and_type(pages_text)\n",
        "\n",
        "        structured_sections = []\n",
        "        preemble = \"\"\n",
        "\n",
        "        full_text = \" \".join(pages_text)\n",
        "\n",
        "        if(doc_type == \"act\"):\n",
        "            act_sections, preemble = self.document_analyzer.extract_act_sections(full_text)\n",
        "            structured_sections = act_sections\n",
        "            preemble = preemble\n",
        "\n",
        "        elif(doc_type == \"case\"):\n",
        "            case_sections, preemble = self.document_analyzer.extract_case_sections(full_text)\n",
        "            structured_sections = case_sections\n",
        "            preemble = preemble\n",
        "\n",
        "        # Print structured sections for debugging\n",
        "        # print(f\"Total pages: {len(pages_text)}\")\n",
        "        # self.document_analyzer.print_structured_sections(structured_sections)\n",
        "\n",
        "        cleaned_text = \"\\n\".join([section[\"text\"] for section in structured_sections])\n",
        "\n",
        "        return {\n",
        "            \"id\": unique_id,\n",
        "            \"type\": doc_type,\n",
        "            \"amendmentTo\": amendment_to,\n",
        "            \"filename\": filename,\n",
        "            \"primaryLang\": primary_lang,\n",
        "            \"title\": document_title.replace(\"Case No. \", \"\").replace(\"\\n\", \" \"),\n",
        "            \"cleanedText\": cleaned_text,\n",
        "            \"removedText\": preemble,\n",
        "            \"wordCount\": len(cleaned_text.split()),\n",
        "            \"pagesCount\": len(pages_text),\n",
        "            \"structuredSections\": structured_sections\n",
        "        }\n",
        "\n",
        "    def process_folder(self, folder_path: str,\n",
        "                      start_index: int = 0, end_index: int = 1) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Process all PDFs in a folder using multiprocessing.\n",
        "\n",
        "        Args:\n",
        "            folder_path: Path to folder containing PDF files\n",
        "            max_workers: Maximum number of worker processes\n",
        "            start_index: Starting index for PDF files to process\n",
        "            end_index: Ending index for PDF files to process\n",
        "\n",
        "        Returns:\n",
        "            List of processed document dictionaries\n",
        "        \"\"\"\n",
        "        results = []\n",
        "\n",
        "        # Get PDF files with slice\n",
        "        pdf_files = [\n",
        "            os.path.join(folder_path, filename)\n",
        "            for filename in os.listdir(folder_path)\n",
        "            if filename.lower().endswith(\".pdf\")\n",
        "        ][start_index:end_index]\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:\n",
        "            futures = {\n",
        "                executor.submit(self.process_pdf_file, pdf_path): pdf_path\n",
        "                for pdf_path in pdf_files\n",
        "            }\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                pdf_path = futures[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    results.append(result)\n",
        "                    # print(f\"Processed: {result['filename']} | \"\n",
        "                    #       f\"Primary Language: {result['primaryLang']} | \"\n",
        "                    #       f\"Word Count: {result['wordCount']} | \"\n",
        "                    #       f\"Pages Count: {result['pagesCount']}\\n\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {pdf_path}: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_summary(self, pdf_data: List[Dict[str, Any]]) -> None:\n",
        "        \"\"\"\n",
        "        Print summary of processed documents.\n",
        "\n",
        "        Args:\n",
        "            pdf_data: List of processed document dictionaries\n",
        "        \"\"\"\n",
        "        for data in pdf_data:\n",
        "            print(f\"ID: {data['id']}\")\n",
        "            print(f\"Filename: {data['filename']}\")\n",
        "            print(f\"Page Count: {data['pagesCount']}\")\n",
        "            print(f\"Primary Language: {data['primaryLang']}\")\n",
        "            print(f\"Title: {data['title']}\")\n",
        "            print(f\"Word Count: {data['wordCount']}\")\n",
        "            print(f\"Type: {data['type']}\")\n",
        "            print(f\"Amendment To: {data['amendmentTo']}\")\n",
        "            print(f\"Structured Sections: {len(data['structuredSections'])}\")\n",
        "            print(f\"\\nText Preview: {data['cleanedText'][:200]}\")\n",
        "            print(f\"\\nRemoved Text: {data['removedText'][:200]}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# ------------------- Main Execution -------------------\n",
        "def main():\n",
        "    \"\"\"Main execution function.\"\"\"\n",
        "    # Configuration\n",
        "    folder_path = \"/content/drive/MyDrive/FYP/law_cases_raw/2024\"\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = DocumentProcessor()\n",
        "\n",
        "    # Process PDFs\n",
        "    pdf_data = processor.process_folder(folder_path, 30, 31)\n",
        "\n",
        "    # Print summary\n",
        "    processor.print_summary(pdf_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLwFF80YjkO4",
        "outputId": "b00e6200-d7ad-4681-c8d6-36048cd928a2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: 69902851-6265-49d2-91f4-af5f58e6a6c2\n",
            "Filename: lta_0001_pdf.pdf\n",
            "Page Count: 5\n",
            "Primary Language: en (0.80)\n",
            "Title: IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIALIST REPUBLIC OF SRI LANKA In the matter of an Application for Leave to Appeal against the Order of the Board of Quazis dated 07/12/2019 in terms of Section 44(3) of the Muslim Marriage and Divorce Act No. 13 of 1951\n",
            "Word Count: 655\n",
            "Type: act\n",
            "Amendment To: \n",
            "Structured Sections: 2\n",
            "\n",
            "Text Preview: We heard the learned Counsel for the\n",
            "Petitioner in support of this application. We heard the learned Counsel\n",
            "for the Applicant-Respondent-Respondent (hereinafter referred to as\n",
            "the Respondent) as well\n",
            "\n",
            "Removed Text: IN THE COURT OF APPEAL OF THE DEMOCRATIC SOCIALIST REPUBLIC\n",
            "OF SRI LANKA\n",
            "In the matter of an Application for Leave to\n",
            "Appeal against the Order of the Board of\n",
            "Quazis dated 07/12/2019 in terms of\n",
            "Secti\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Translation\n",
        "\n",
        "import re\n",
        "import fasttext\n",
        "import asyncio\n",
        "from googletrans import Translator\n",
        "import nest_asyncio  # For Jupyter notebook environments\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Load FastText model\n",
        "model = fasttext.load_model(\"lid.176.bin\")\n",
        "\n",
        "# Initialize Google Translate API\n",
        "translator = Translator()\n",
        "\n",
        "# Modified detect_language_fasttext to process chunks sequentially\n",
        "async def detect_language_fasttext(text, word_threshold=300):\n",
        "    \"\"\"Detects if the text contains non-English content.\"\"\"\n",
        "    words = text.split()\n",
        "    total_words = len(words)\n",
        "    num_chunks = max(1, total_words // word_threshold)\n",
        "\n",
        "    # Process chunks sequentially to avoid asyncio.as_completed issue\n",
        "    for i in range(num_chunks):\n",
        "        chunk = \" \".join(words[i * word_threshold:(i + 1) * word_threshold])\n",
        "        try:\n",
        "            # Call predict directly without asyncio.to_thread\n",
        "            prediction = model.predict(chunk)\n",
        "\n",
        "            # Ensure prediction has the expected structure before accessing elements\n",
        "            if prediction and len(prediction) > 0 and len(prediction[0]) > 0:\n",
        "                detected_lang = prediction[0][0].replace(\"__label__\", \"\")\n",
        "                if detected_lang != \"en\":\n",
        "                    print(f\"Chunk needs translation (detected: {detected_lang})\")\n",
        "                    return True  # Indicates translation is needed\n",
        "\n",
        "            else:\n",
        "                 print(\"Warning: Received empty or unexpected prediction format for a chunk.\")\n",
        "\n",
        "        except ValueError as e:\n",
        "             # Log the specific ValueError if it still occurs within predict\n",
        "             if \"Unable to avoid copy while creating an array as requested\" in str(e):\n",
        "                 print(f\"Caught ValueError during fasttext.predict: {e}\")\n",
        "                 # Continue to the next chunk or handle as needed\n",
        "                 pass # Or return True to force translation on error\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fasttext prediction for a chunk: {e}\")\n",
        "            # Decide how to handle other errors, e.g., force translation\n",
        "            # return True\n",
        "\n",
        "    return False  # No translation needed\n",
        "\n",
        "\n",
        "async def translate_if_needed(text, max_length=2000):\n",
        "    \"\"\"Translates text while preserving sentence boundaries asynchronously.\"\"\"\n",
        "    # Await the simplified language detection\n",
        "    if await detect_language_fasttext(text):\n",
        "        try:\n",
        "            # Split text by sentence boundaries (., !, ?, newline)\n",
        "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "            chunks = []\n",
        "            current_chunk = \"\"\n",
        "\n",
        "            for sentence in sentences:\n",
        "                # Ensure sentence is not empty after split\n",
        "                if not sentence.strip():\n",
        "                    continue\n",
        "\n",
        "                # Check if adding the next sentence exceeds max_length\n",
        "                if len(current_chunk) + len(sentence) + (1 if current_chunk else 0) < max_length:\n",
        "                    current_chunk += (sentence + \" \").strip() if current_chunk else sentence.strip()\n",
        "                else:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                    current_chunk = sentence.strip() + \" \"\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "\n",
        "            print(f\"Translating {len(chunks)} chunks.\")\n",
        "            # Translate all chunks in parallel using asyncio.gather\n",
        "            tasks = [asyncio.to_thread(translator.translate, chunk, dest='en', src='si') for chunk in chunks]\n",
        "            translated_chunks = await asyncio.gather(*tasks)\n",
        "\n",
        "            # Extract translated text\n",
        "            translated_texts = [tr.text for tr in translated_chunks]\n",
        "            print(\"Translation complete.\")\n",
        "\n",
        "            return \" \".join(translated_texts)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text  # Return original if translation fails\n",
        "\n",
        "    return text  # Return original if no translation is needed\n",
        "\n",
        "async def process_documents(pdf_data):\n",
        "    \"\"\"Processes documents asynchronously in parallel.\"\"\"\n",
        "    print(f\"Starting translation for {len(pdf_data)} documents.\")\n",
        "    tasks = []\n",
        "    for doc in pdf_data:\n",
        "        # Pass the entire cleanedText to translate_if_needed\n",
        "        tasks.append(translate_if_needed(doc.get(\"cleanedText\", \"\")))\n",
        "\n",
        "    # Run translations in parallel\n",
        "    translated_texts = await asyncio.gather(*tasks)\n",
        "\n",
        "    # Assign translated text back to documents\n",
        "    for i, doc in enumerate(pdf_data):\n",
        "        doc[\"text\"] = translated_texts[i]\n",
        "\n",
        "    print(\"Translation process finished.\")\n",
        "    # Print a preview of the updated text\n",
        "    for doc in pdf_data:\n",
        "        print(f\"ID: {doc['id']}\\nFilename: {doc['filename']}\\nWord Count:{len(doc['text'].split())}\\nText Preview: {doc['text'][:200]}\\n{'-'*50}\\n\")\n",
        "\n",
        "# Main function to run process_documents\n",
        "async def main():\n",
        "    await process_documents(pdf_data)\n",
        "\n",
        "# Run the main function in an environment with an existing event loop\n",
        "try:\n",
        "    loop = asyncio.get_running_loop()  # Get the current running loop\n",
        "except RuntimeError:  # No running event loop, create a new one\n",
        "    loop = asyncio.new_event_loop()\n",
        "    asyncio.set_event_loop(loop)\n",
        "\n",
        "# Await the main function (ensuring all tasks finish)\n",
        "if loop.is_running():\n",
        "    # Use asyncio.run if running in a script or ensure a loop is already running\n",
        "    # In Colab, a loop is usually running, so create_task and await is appropriate\n",
        "    task = asyncio.create_task(main())\n",
        "    await task\n",
        "else:\n",
        "    loop.run_until_complete(main())"
      ],
      "metadata": {
        "id": "AdpHsFJoM82x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3773e6-52e8-4329-ce1e-7e11d8f86fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting translation for 5 documents.\n",
            "Translation process finished.\n",
            "ID: 8da3df2b-69dd-4273-b6eb-f88a5dd1f7f5\n",
            "Filename: cpa_0132_23_final_judgement_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: f18e71a3-c2c1-444e-a2cd-b30ba2b96656\n",
            "Filename: ca_writ_170_22_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: a362d0b8-7941-4a9f-b988-4e2f989f6056\n",
            "Filename: court_of_appeal_judgment_hcc_0184_17_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: f38c9d03-2598-4ef4-93ca-c78782c2bae9\n",
            "Filename: wrt_0201_21_31_01_2024_1_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n",
            "ID: 2386535a-29de-49b8-b920-9654640ab23d\n",
            "Filename: writ_80_2018_judgment_1_new_finally_adjusted_corrected_foot_note_completed_pdf.pdf\n",
            "Word Count:0\n",
            "Text Preview: \n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the output file path\n",
        "output_file = \"/content/drive/MyDrive/FYP/json/cases_2024_v2.json\"\n",
        "\n",
        "# delete if exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Write the pdf_data to a JSON file\n",
        "with open(output_file, \"w\", encoding='utf-8') as f:\n",
        "    json.dump(pdf_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"PDF data successfully written to {output_file}\")\n"
      ],
      "metadata": {
        "id": "QjraXEFXyqkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4c90f7-4537-466d-acb5-48449df0820f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF data successfully written to /content/drive/MyDrive/FYP/json/acts_2024.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}